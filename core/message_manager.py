"""
æ¶ˆæ¯æ¨é€ç®¡ç†å™¨
å®ç°ä¼˜åŒ–çš„æŠ¥æ•°æ§åˆ¶ã€æ‹†åˆ†è¿‡æ»¤å™¨å’Œæ”¹è¿›çš„å»é‡é€»è¾‘
"""

import json
import os
import urllib.parse
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from typing import Any
import tempfile
from jinja2 import Template
from playwright.async_api import async_playwright

import astrbot.api.message_components as Comp
from astrbot.api import logger
from astrbot.api.event import MessageChain
from astrbot.api.star import StarTools
from astrbot.core.utils.t2i.renderer import HtmlRenderer

from ..models.data_source_config import (
    get_intensity_based_sources,
    get_scale_based_sources,
)
from ..models.models import (
    DataSource,
    DisasterEvent,
    EarthquakeData,
    TsunamiData,
    WeatherAlarmData,
)
from ..utils.formatters import (
    BaseMessageFormatter,
    GlobalQuakeFormatter,
    format_earthquake_message,
    format_tsunami_message,
    format_weather_message,
)
from .event_deduplicator import EventDeduplicator
from .filters import (
    GlobalQuakeFilter,
    IntensityFilter,
    LocalIntensityFilter,
    ReportCountController,
    ScaleFilter,
    USGSFilter,
    WeatherFilter,
)


class MessagePushManager:
    """æ¶ˆæ¯æ¨é€ç®¡ç†å™¨"""

    def __init__(self, config: dict[str, Any], context):
        self.config = config
        self.context = context

        # åˆå§‹åŒ–è¿‡æ»¤å™¨ - ä½¿ç”¨æ–°çš„é…ç½®è·¯å¾„
        earthquake_filters = config.get("earthquake_filters", {})

        # çƒˆåº¦è¿‡æ»¤å™¨é…ç½®
        intensity_filter_config = earthquake_filters.get("intensity_filter", {})
        self.intensity_filter = IntensityFilter(
            enabled=intensity_filter_config.get("enabled", True),
            min_magnitude=intensity_filter_config.get("min_magnitude", 2.0),
            min_intensity=intensity_filter_config.get("min_intensity", 4.0),
        )

        # éœ‡åº¦è¿‡æ»¤å™¨é…ç½®
        scale_filter_config = earthquake_filters.get("scale_filter", {})
        self.scale_filter = ScaleFilter(
            enabled=scale_filter_config.get("enabled", True),
            min_magnitude=scale_filter_config.get("min_magnitude", 2.0),
            min_scale=scale_filter_config.get("min_scale", 1.0),
        )

        # USGSè¿‡æ»¤å™¨é…ç½®
        magnitude_only_filter_config = earthquake_filters.get(
            "magnitude_only_filter", {}
        )
        self.usgs_filter = USGSFilter(
            enabled=magnitude_only_filter_config.get("enabled", True),
            min_magnitude=magnitude_only_filter_config.get("min_magnitude", 4.5),
        )

        # Global Quakeè¿‡æ»¤å™¨é…ç½®
        global_quake_filter_config = earthquake_filters.get("global_quake_filter", {})
        self.global_quake_filter = GlobalQuakeFilter(
            enabled=global_quake_filter_config.get("enabled", True),
            min_magnitude=global_quake_filter_config.get("min_magnitude", 4.5),
            min_intensity=global_quake_filter_config.get("min_intensity", 5.0),
        )

        # åˆå§‹åŒ–æŠ¥æ•°æ§åˆ¶å™¨
        push_config = config.get("push_frequency_control", {})
        self.report_controller = ReportCountController(
            cea_cwa_report_n=push_config.get("cea_cwa_report_n", 1),
            jma_report_n=push_config.get("jma_report_n", 3),
            gq_report_n=push_config.get("gq_report_n", 5),
            final_report_always_push=push_config.get("final_report_always_push", True),
            ignore_non_final_reports=push_config.get("ignore_non_final_reports", False),
        )

        # åˆå§‹åŒ–å»é‡å™¨
        self.deduplicator = EventDeduplicator(
            time_window_minutes=config.get("event_deduplication", {}).get(
                "time_window_minutes", 1
            ),
            location_tolerance_km=config.get("event_deduplication", {}).get(
                "location_tolerance_km", 20.0
            ),
            magnitude_tolerance=config.get("event_deduplication", {}).get(
                "magnitude_tolerance", 0.5
            ),
        )

        # äº‹ä»¶æ¨é€è®°å½•
        self.event_push_records: dict[str, list[dict]] = defaultdict(list)

        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_dir = StarTools.get_data_dir("astrbot_plugin_disaster_warning")
        self.stats_file = self.data_dir / "push_stats.json"

        # åŠ è½½å†å²è®°å½•
        self._load_stats()

        # ç›®æ ‡ä¼šè¯
        self.target_sessions = self._parse_target_sessions()

        # åˆå§‹åŒ–æœ¬åœ°ç›‘æ§è¿‡æ»¤å™¨
        self.local_monitor = LocalIntensityFilter(config.get("local_monitoring", {}))

        # åˆå§‹åŒ–æ°”è±¡é¢„è­¦è¿‡æ»¤å™¨
        weather_filter_config = (
            config.get("data_sources", {})
            .get("fan_studio", {})
            .get("weather_filter", {})
        )
        self.weather_filter = WeatherFilter(weather_filter_config)

    def _parse_target_sessions(self) -> list[str]:
        """è§£æç›®æ ‡ä¼šè¯ - ä½¿ç”¨æ­£ç¡®çš„é…ç½®é”®å"""
        target_groups = self.config.get("target_groups", [])
        sessions = []

        for group_id in target_groups:
            if group_id:
                # ä½¿ç”¨æ­£ç¡®çš„ä¼šè¯IDæ ¼å¼
                platform_name = self.config.get("platform_name", "aiocqhttp")
                session = f"{platform_name}:GroupMessage:{group_id}"
                sessions.append(session)

        return sessions

    def should_push_event(self, event: DisasterEvent) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¨é€äº‹ä»¶"""
        # 1. æ—¶é—´æ£€æŸ¥ï¼ˆæ‰€æœ‰äº‹ä»¶ç±»å‹ï¼‰- è¿™æ˜¯æœ€é‡è¦çš„è¿‡æ»¤
        # è·å–å¸¦æ—¶åŒºçš„äº‹ä»¶æ—¶é—´
        event_time_aware = self._get_event_time(event)

        if event_time_aware:
            # ä½¿ç”¨UTCå½“å‰æ—¶é—´è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿æ—¶åŒºæ— å…³æ€§
            current_time_utc = datetime.now(timezone.utc)
            time_diff = (
                current_time_utc - event_time_aware
            ).total_seconds() / 3600  # å°æ—¶

            if time_diff > 1:
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶æ—¶é—´è¿‡æ—©ï¼ˆ{time_diff:.1f}å°æ—¶å‰ï¼‰ï¼Œè¿‡æ»¤")
                return False

        # 2. éåœ°éœ‡äº‹ä»¶æ£€æŸ¥
        if not isinstance(event.data, EarthquakeData):
            # æ°”è±¡é¢„è­¦äº‹ä»¶éœ€è¦è¿›è¡Œè¿‡æ»¤
            if isinstance(event.data, WeatherAlarmData):
                headline = event.data.headline or event.data.title or ""
                if self.weather_filter.should_filter(headline):
                    return False
            # æµ·å•¸å’Œæ°”è±¡äº‹ä»¶é€šè¿‡äº†è¿‡æ»¤ï¼Œå¯ä»¥æ¨é€
            return True

        # 3. åœ°éœ‡äº‹ä»¶ä¸“ç”¨è¿‡æ»¤é€»è¾‘
        earthquake = event.data
        source_id = self._get_source_id(event)

        # æ•°æ®æºä¸“ç”¨è¿‡æ»¤å™¨
        if source_id == "global_quake":
            # Global Quakeä¸“ç”¨è¿‡æ»¤å™¨
            if self.global_quake_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«Global Quakeè¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id in get_intensity_based_sources():
            # ä½¿ç”¨çƒˆåº¦è¿‡æ»¤å™¨
            if self.intensity_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«çƒˆåº¦è¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id in get_scale_based_sources():
            # ä½¿ç”¨éœ‡åº¦è¿‡æ»¤å™¨
            if self.scale_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«éœ‡åº¦è¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id == "usgs_fanstudio":
            # USGSä¸“ç”¨è¿‡æ»¤å™¨
            if self.usgs_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«USGSè¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False

        # æŠ¥æ•°æ§åˆ¶ï¼ˆä»…EEWæ•°æ®æºï¼‰
        if not self.report_controller.should_push_report(event):
            logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«æŠ¥æ•°æ§åˆ¶å™¨è¿‡æ»¤: {source_id}")
            return False

        # æœ¬åœ°çƒˆåº¦è¿‡æ»¤ä¸æ³¨å…¥ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è¾…åŠ©æ–¹æ³•ï¼‰
        result = self.local_monitor.inject_local_estimation(earthquake)
        # result ä¸º None è¡¨ç¤ºæœªå¯ç”¨ï¼Œå¦åˆ™æ£€æŸ¥ is_allowed
        if result is not None and not result.get("is_allowed", True):
            return False

        return True

    def _get_event_time(self, event: DisasterEvent) -> datetime | None:
        """è·å–ç¾å®³äº‹ä»¶çš„å¸¦æ—¶åŒºæ—¶é—´ (Aware Datetime)"""
        raw_time = None
        if isinstance(event.data, EarthquakeData):
            raw_time = event.data.shock_time
        elif isinstance(event.data, TsunamiData):
            raw_time = event.data.issue_time
        elif isinstance(event.data, WeatherAlarmData):
            raw_time = event.data.effective_time or event.data.issue_time

        if not raw_time:
            return None

        # å¦‚æœå·²ç»æ˜¯Awareæ—¶é—´ï¼Œç›´æ¥è¿”å›
        if raw_time.tzinfo is not None:
            return raw_time

        # æ ¹æ®æ•°æ®æºIDç¡®å®šæ—¶åŒº
        source_id = event.source_id or self._get_source_id(event)

        # å®šä¹‰æ—¶åŒº
        # JST (UTC+9)
        tz_jst = timezone(timedelta(hours=9))
        # CST (UTC+8)
        tz_cst = timezone(timedelta(hours=8))
        # UTC
        tz_utc = timezone.utc

        # 1. UTC+9 æ•°æ®æº
        # - Fan Studio JMA
        # - P2P Quake (æ‰€æœ‰)
        # - Wolfx JMA
        if (
            "jma" in source_id
            or "p2p" in source_id
            or source_id == "wolfx_jma_eew"
            or source_id == "wolfx_jma_eq"
        ):
            return raw_time.replace(tzinfo=tz_jst)

        # 2. UTC æ•°æ®æº
        # - Global Quake
        if "global_quake" in source_id:
            return raw_time.replace(tzinfo=tz_utc)

        # 3. UTC+8 æ•°æ®æº (é»˜è®¤)
        # - Fan Studio (é™¤äº† JMA, USGSå·²è½¬ä¸ºUTC+8)
        # - Wolfx (é™¤äº† JMA)
        # - China Weather/Tsunami
        return raw_time.replace(tzinfo=tz_cst)

    def _get_source_id(self, event: DisasterEvent) -> str:
        """è·å–äº‹ä»¶çš„æ•°æ®æºID"""
        source_mapping = {
            # EEWé¢„è­¦æ•°æ®æº
            DataSource.FAN_STUDIO_CEA.value: "cea_fanstudio",
            DataSource.WOLFX_CENC_EEW.value: "cea_wolfx",
            DataSource.FAN_STUDIO_CWA.value: "cwa_fanstudio",
            DataSource.WOLFX_CWA_EEW.value: "cwa_wolfx",
            DataSource.FAN_STUDIO_JMA.value: "jma_fanstudio",
            DataSource.P2P_EEW.value: "jma_p2p",
            DataSource.WOLFX_JMA_EEW.value: "jma_wolfx",
            # åœ°éœ‡æƒ…æŠ¥æ•°æ®æº
            DataSource.FAN_STUDIO_CENC.value: "cenc_fanstudio",
            DataSource.WOLFX_CENC_EQ.value: "cenc_wolfx",
            DataSource.P2P_EARTHQUAKE.value: "jma_p2p_info",
            DataSource.WOLFX_JMA_EQ.value: "jma_wolfx_info",
            DataSource.FAN_STUDIO_USGS.value: "usgs_fanstudio",
            DataSource.GLOBAL_QUAKE.value: "global_quake",
            # æ°”è±¡å’Œæµ·å•¸é¢„è­¦æ•°æ®æº
            DataSource.FAN_STUDIO_WEATHER.value: "china_weather_fanstudio",
            DataSource.FAN_STUDIO_TSUNAMI.value: "china_tsunami_fanstudio",
            DataSource.P2P_TSUNAMI.value: "jma_tsunami_p2p",
        }

        return source_mapping.get(event.source.value, event.source.value)

    async def push_event(self, event: DisasterEvent) -> bool:
        """æ¨é€äº‹ä»¶"""
        logger.debug(f"[ç¾å®³é¢„è­¦] å¤„ç†äº‹ä»¶æ¨é€: {event.id}")

        # 1. å…ˆå»é‡æ£€æŸ¥ - å…è®¸å¤šæ•°æ®æºæ¨é€åŒä¸€äº‹ä»¶
        if not self.deduplicator.should_push_event(event):
            logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} è¢«å»é‡å™¨è¿‡æ»¤")
            return False

        # 2. æ¨é€æ¡ä»¶æ£€æŸ¥
        if not self.should_push_event(event):
            logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} æœªé€šè¿‡æ¨é€æ¡ä»¶æ£€æŸ¥")
            return False

        try:
            # 3. æ„å»ºæ¶ˆæ¯ (ä½¿ç”¨å¼‚æ­¥æ„å»ºä»¥æ”¯æŒå¡ç‰‡æ¸²æŸ“)
            message = await self._build_message_async(event)
            logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯æ„å»ºå®Œæˆ")

            # 4. è·å–ç›®æ ‡ä¼šè¯
            target_sessions = self.target_sessions
            if not target_sessions:
                logger.warning("[ç¾å®³é¢„è­¦] æ²¡æœ‰é…ç½®ç›®æ ‡ä¼šè¯ï¼Œæ— æ³•æ¨é€æ¶ˆæ¯")
                return False

            # 5. æ¨é€æ¶ˆæ¯
            push_success_count = 0
            for session in target_sessions:
                try:
                    await self._send_message(session, message)
                    logger.info(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯å·²æ¨é€åˆ° {session}")
                    push_success_count += 1
                except Exception as e:
                    logger.error(f"[ç¾å®³é¢„è­¦] æ¨é€åˆ° {session} å¤±è´¥: {e}")

            # 6. è®°å½•æ¨é€
            self._record_push(event)
            logger.info(
                f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} æ¨é€å®Œæˆï¼ŒæˆåŠŸæ¨é€åˆ° {push_success_count} ä¸ªä¼šè¯"
            )
            return push_success_count > 0

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ¨é€äº‹ä»¶å¤±è´¥: {e}")
            return False

    def _build_message(self, event: DisasterEvent) -> MessageChain:
        """æ„å»ºæ¶ˆæ¯ - ä½¿ç”¨æ ¼å¼åŒ–å™¨å¹¶åº”ç”¨æ¶ˆæ¯æ ¼å¼é…ç½®ï¼ˆå‘åå…¼å®¹ï¼Œä»…è°ƒç”¨åŒæ­¥é€»è¾‘ï¼‰"""
        source_id = self._get_source_id(event)
        message_format_config = self.config.get("message_format", {})
        return self._build_message_sync(
            event,
            source_id,
            message_format_config.get("include_map", True),
            message_format_config.get("map_provider", "baidu"),
            message_format_config.get("map_zoom_level", 5),
            message_format_config.get("detailed_jma_intensity", False),
        )

    async def _build_message_async(self, event: DisasterEvent) -> MessageChain:
        """æ„å»ºæ¶ˆæ¯ (å¼‚æ­¥ç‰ˆæœ¬) - æ”¯æŒå¡ç‰‡æ¸²æŸ“"""
        source_id = self._get_source_id(event)
        message_format_config = self.config.get("message_format", {})
        use_gq_card = message_format_config.get("use_global_quake_card", False)

        if (
            source_id == "global_quake"
            and use_gq_card
            and isinstance(event.data, EarthquakeData)
        ):
            try:
                # æ¸²æŸ“ Global Quake å¡ç‰‡
                context = GlobalQuakeFormatter.get_render_context(event.data)

                # åŠ è½½æ¨¡æ¿
                current_file_dir = os.path.dirname(os.path.abspath(__file__))
                resources_dir = os.path.join(
                    os.path.dirname(current_file_dir), "resources"
                )
                template_path = os.path.join(resources_dir, "global_quake_card.html")

                if not os.path.exists(template_path):
                    logger.error(f"[ç¾å®³é¢„è­¦] æ‰¾ä¸åˆ°æ¨¡æ¿æ–‡ä»¶: {template_path}")
                    # å›é€€åˆ°åŒæ­¥æ„å»º
                    return self._build_message_sync(
                        event,
                        source_id,
                        message_format_config.get("include_map", True),
                        message_format_config.get("map_provider", "baidu"),
                        message_format_config.get("map_zoom_level", 5),
                        message_format_config.get("detailed_jma_intensity", False),
                    )

                with open(template_path, encoding="utf-8") as f:
                    template_content = f.read()
                
                # Jinja2 æ¸²æŸ“
                template = Template(template_content)
                html_content = template.render(**context)

                # ä½¿ç”¨ Playwright æ¸²æŸ“
                async with async_playwright() as p:
                    browser = await p.chromium.launch(
                        args=['--no-sandbox', '--disable-setuid-sandbox']
                    )
                    
                    # åˆ›å»ºæ–°é¡µé¢ï¼Œè§†å£è®¾ç½®å¤§ä¸€ç‚¹å‡å¯ï¼Œå› ä¸ºæˆ‘ä»¬åªæˆªå–å…ƒç´ 
                    # å…³é”®ä¿®å¤ï¼šè®¾ç½® device_scale_factor=3 æé«˜æ¸²æŸ“DPIï¼Œè§£å†³å›¾ç‰‡æ¨¡ç³Šé—®é¢˜
                    page = await browser.new_page(
                        viewport={"width": 800, "height": 800},
                        device_scale_factor=3
                    )
                    
                    await page.set_content(html_content)

                    # ç­‰å¾…å…ƒç´ åŠ è½½
                    await page.wait_for_load_state("networkidle")
                    
                    # å…³é”®ä¿®å¤ï¼šç­‰å¾… D3 æ¸²æŸ“å®Œæˆæ ‡è®°
                    try:
                        await page.wait_for_selector(".d3-ready", state="attached", timeout=5000)
                    except Exception:
                        # å¦‚æœè¶…æ—¶ï¼ˆä¾‹å¦‚JSæŠ¥é”™ï¼‰ï¼Œä¹Ÿä¸è¦å´©æºƒï¼Œå°½åŠ›è€Œä¸ºæˆªå›¾
                        pass

                    # ç»Ÿä¸€ä½¿ç”¨ ID é€‰æ‹©å™¨ï¼Œè¿™åœ¨æ‰€æœ‰æ¨¡æ¿ä¸­éƒ½å°†é€šç”¨
                    selector = "#card-wrapper"
                    try:
                        await page.wait_for_selector(selector, state="visible", timeout=5000)
                    except Exception:
                        # å…œåº•ï¼šå°è¯•æ‰¾å¸¸è§çš„ç±»å
                        selector = ".quake-card"
                        await page.wait_for_selector(selector, state="visible", timeout=2000)
                    
                    # å®šä½å¡ç‰‡å…ƒç´ 
                    card = page.locator(selector)
                    
                    # å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                    temp_dir = os.path.join(os.path.dirname(self.data_dir), "temp")
                    if not os.path.exists(temp_dir):
                        os.makedirs(temp_dir, exist_ok=True)
                    
                    image_filename = f"gq_card_{event.data.id}_{int(datetime.now().timestamp())}.png"
                    image_path = os.path.join(temp_dir, image_filename)
                    
                    # æˆªå›¾ï¼šåªæˆªå–å…ƒç´ ï¼ŒèƒŒæ™¯é€æ˜
                    await card.screenshot(path=image_path, omit_background=True)
                    
                    await browser.close()

                    if os.path.exists(image_path):
                        logger.info(
                            f"[ç¾å®³é¢„è­¦] Global Quake å¡ç‰‡æ¸²æŸ“æˆåŠŸ: {image_path}"
                        )
                        chain = [Comp.Image.fromFileSystem(image_path)]
                        return MessageChain(chain)
                    else:
                        logger.warning(
                            "[ç¾å®³é¢„è­¦] Global Quake å¡ç‰‡æ¸²æŸ“æœªç”Ÿæˆæ–‡ä»¶"
                        )

            except Exception as e:
                logger.error(
                    f"[ç¾å®³é¢„è­¦] Global Quake å¡ç‰‡æ¸²æŸ“å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼"
                )

        # é»˜è®¤å›é€€åˆ°åŒæ­¥æ„å»ºé€»è¾‘
        return self._build_message_sync(
            event,
            source_id,
            message_format_config.get("include_map", True),
            message_format_config.get("map_provider", "baidu"),
            message_format_config.get("map_zoom_level", 5),
            message_format_config.get("detailed_jma_intensity", False),
        )

    def _build_message_sync(
        self, event, source_id, include_map, map_provider, map_zoom_level, detailed_jma
    ) -> MessageChain:
        """åŒæ­¥æ„å»ºæ¶ˆæ¯é€»è¾‘ï¼ˆåŸ _build_message å†…å®¹ï¼‰"""
        if isinstance(event.data, WeatherAlarmData):
            message_text = format_weather_message(source_id, event.data)
        elif isinstance(event.data, TsunamiData):
            message_text = format_tsunami_message(source_id, event.data)
        elif isinstance(event.data, EarthquakeData):
            # ä¼ é€’é…ç½®é€‰é¡¹
            options = {"detailed_jma_intensity": detailed_jma}
            message_text = format_earthquake_message(source_id, event.data, options)
        else:
            # æœªçŸ¥äº‹ä»¶ç±»å‹ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–
            logger.warning(f"[ç¾å®³é¢„è­¦] æœªçŸ¥äº‹ä»¶ç±»å‹: {type(event.data)}")
            message_text = f"ğŸš¨[æœªçŸ¥äº‹ä»¶]\nğŸ“‹äº‹ä»¶IDï¼š{event.id}\nâ°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        # æ„å»ºæ¶ˆæ¯é“¾
        if include_map and isinstance(event.data, EarthquakeData):
            if event.data.latitude is not None and event.data.longitude is not None:
                # ä½¿ç”¨æ¶ˆæ¯æ ¼å¼åŒ–å™¨ä¸­çš„ä¼˜åŒ–åœ°å›¾é“¾æ¥ç”Ÿæˆ
                map_url = BaseMessageFormatter.get_map_link(
                    event.data.latitude,
                    event.data.longitude,
                    map_provider,
                    map_zoom_level,
                    magnitude=event.data.magnitude,
                    place_name=event.data.place_name,
                )
                if map_url:
                    # å…³é”®ä¿®å¤ï¼šç»•è¿‡AstrBotçš„strip()é—®é¢˜
                    # ä½¿ç”¨é›¶å®½ç©ºæ ¼ä¿æŠ¤æ¢è¡Œï¼ŒURLç¼–ç ç¡®ä¿ç‰¹æ®Šå­—ç¬¦å¤„ç†
                    zero_width_space = "\u200b"
                    encoded_map_url = urllib.parse.quote(map_url, safe=":/?&=+")

                    # ç›´æ¥åˆå¹¶åˆ°æ¶ˆæ¯æ–‡æœ¬ä¸­
                    message_text += f"{zero_width_space}\nğŸ—ºï¸åœ°å›¾é“¾æ¥:{zero_width_space} {encoded_map_url}"

        # æ„å»ºæ¶ˆæ¯é“¾
        chain = [Comp.Plain(message_text)]
        return MessageChain(chain)

    def _generate_map_link(
        self, latitude: float, longitude: float, provider: str, zoom: int
    ) -> str:
        """æ ¹æ®é…ç½®ç”Ÿæˆåœ°å›¾é“¾æ¥ - å·²ç§»è‡³message_formattersæ¨¡å—"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ç”±message_formattersæ¨¡å—å¤„ç†
        return BaseMessageFormatter.get_map_link(
            latitude,
            longitude,
            provider,
            zoom,
            magnitude=None,  # è¿™ä¸ªæ–¹æ³•æ²¡æœ‰éœ‡çº§ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            place_name=None,  # è¿™ä¸ªæ–¹æ³•æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        )

    async def _send_message(self, session: str, message: MessageChain):
        """å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šä¼šè¯"""
        await self.context.send_message(session, message)

    def _record_push(self, event: DisasterEvent):
        """è®°å½•æ¨é€"""
        event_id = self._get_event_id(event)

        # è®°å½•æ¨é€ä¿¡æ¯
        push_info = {
            "timestamp": datetime.now().isoformat(),  # ä½¿ç”¨ISOæ ¼å¼ä»¥ä¾¿åºåˆ—åŒ–
            "event_id": event_id,
            "disaster_type": event.disaster_type.value,
            "source": self._get_source_id(event),
        }

        self.event_push_records[event_id].append(push_info)

        # ä¿å­˜ç»Ÿè®¡æ•°æ® (æ¯æ¬¡æ¨é€éƒ½ä¿å­˜ï¼Œä¿è¯æ•°æ®å®‰å…¨)
        self.save_stats()

    def _get_event_id(self, event: DisasterEvent) -> str:
        """è·å–äº‹ä»¶ID"""
        if isinstance(event.data, EarthquakeData):
            return event.data.event_id or event.data.id
        elif isinstance(event.data, (TsunamiData, WeatherAlarmData)):
            return event.data.id
        return event.id

    def get_push_stats(self) -> dict[str, Any]:
        """è·å–æ¨é€ç»Ÿè®¡ - åŒ…å«ç»†åˆ†ç»Ÿè®¡"""
        total_events = len(self.event_push_records)
        total_pushes = 0

        # ç»†åˆ†ç»Ÿè®¡åˆå§‹åŒ–
        breakdown = {"earthquake": 0, "tsunami": 0, "weather": 0, "unknown": 0}

        # æ•°æ®æºç»Ÿè®¡
        source_stats = defaultdict(int)

        for records in self.event_push_records.values():
            if not records:
                continue

            # ä½¿ç”¨ç¬¬ä¸€æ¡è®°å½•çš„ä¿¡æ¯æ¥ç¡®å®šç±»å‹ï¼ˆåŒä¸€IDé€šå¸¸ç±»å‹ç›¸åŒï¼‰
            first_record = records[0]
            dtype = first_record.get("disaster_type", "unknown")
            source = first_record.get("source", "unknown")

            # ç»Ÿè®¡æ¨é€æ€»æ•°
            count = len(records)
            total_pushes += count

            # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
            if dtype in breakdown:
                breakdown[dtype] += count
            else:
                breakdown["unknown"] += count

            # æ›´æ–°æ•°æ®æºç»Ÿè®¡
            source_stats[source] += count

        return {
            "total_events": total_events,
            "total_pushes": total_pushes,
            "breakdown": breakdown,
            "source_stats": dict(source_stats),
            "recent_events": self._get_recent_events(),
        }

    def _get_recent_events(self, hours: int = 24) -> list[dict]:
        """è·å–æœ€è¿‘çš„äº‹ä»¶"""
        recent_time = datetime.now() - timedelta(hours=hours)
        recent_events = []

        for event_id, records in self.event_push_records.items():
            # è½¬æ¢æ—¶é—´æˆ³å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡è¿›è¡Œæ¯”è¾ƒ
            recent_records = []
            for record in records:
                try:
                    ts = datetime.fromisoformat(record["timestamp"])
                    if ts > recent_time:
                        recent_records.append(record)
                except (ValueError, TypeError):
                    continue

            if recent_records:
                # è·å–æœ€åæ¨é€æ—¶é—´
                last_push_strs = [r["timestamp"] for r in recent_records]
                last_push = max(last_push_strs)

                recent_events.append(
                    {
                        "event_id": event_id,
                        "push_count": len(recent_records),
                        "last_push": last_push,
                    }
                )

        return sorted(recent_events, key=lambda x: x["last_push"], reverse=True)

    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            self.save_stats()
        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] è‡ªåŠ¨ä¿å­˜ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®ï¼ˆå…¬å¼€æ–¹æ³•ï¼‰"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.data_dir.mkdir(parents=True, exist_ok=True)

            # å°† defaultdict è½¬æ¢ä¸ºæ™®é€š dict ä¿å­˜
            data_to_save = {
                "records": dict(self.event_push_records),
                "updated_at": datetime.now().isoformat(),
            }

            with open(self.stats_file, "w", encoding="utf-8") as f:
                json.dump(data_to_save, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] ä¿å­˜æ¨é€ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")

    def _load_stats(self):
        """åŠ è½½ç»Ÿè®¡æ•°æ®"""
        try:
            if not self.stats_file.exists():
                return

            with open(self.stats_file, encoding="utf-8") as f:
                data = json.load(f)

            records = data.get("records", {})

            # æ¢å¤æ•°æ®
            for event_id, event_records in records.items():
                self.event_push_records[event_id] = event_records

            logger.info(
                f"[ç¾å®³é¢„è­¦] å·²åŠ è½½ {len(self.event_push_records)} æ¡å†å²æ¨é€è®°å½•"
            )

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] åŠ è½½æ¨é€ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            # å‡ºé”™æ—¶ä¸è¦†ç›–ç°æœ‰æ•°æ®ï¼Œä¿æŒç©ºçŠ¶æ€æˆ–å½“å‰çŠ¶æ€

    def cleanup_old_records(self, days: int = 7):
        """æ¸…ç†æ—§è®°å½•"""
        cutoff_time = datetime.now() - timedelta(days=days)

        # æ¸…ç†äº‹ä»¶æ¨é€è®°å½•
        cleaned_count = 0
        for event_id in list(self.event_push_records.keys()):
            records = self.event_push_records[event_id]

            # è¿‡æ»¤ä¿ç•™æœ€è¿‘çš„è®°å½•
            new_records = []
            for record in records:
                try:
                    ts = datetime.fromisoformat(record["timestamp"])
                    if ts > cutoff_time:
                        new_records.append(record)
                except (ValueError, TypeError):
                    continue

            if new_records:
                self.event_push_records[event_id] = new_records
            else:
                del self.event_push_records[event_id]
                cleaned_count += 1

        # æ¸…ç†å»é‡å™¨
        self.deduplicator.cleanup_old_events()

        # ä¿å­˜æ¸…ç†åçš„ç»“æœ
        self.save_stats()

        logger.info(
            f"[ç¾å®³é¢„è­¦] å·²æ¸…ç† {days} å¤©å‰çš„æ¨é€è®°å½•ï¼Œç§»é™¤ {cleaned_count} ä¸ªè¿‡æœŸäº‹ä»¶"
        )
