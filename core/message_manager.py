"""
æ¶ˆæ¯æ¨é€ç®¡ç†å™¨
å®ç°ä¼˜åŒ–çš„æŠ¥æ•°æ§åˆ¶ã€æ‹†åˆ†è¿‡æ»¤å™¨å’Œæ”¹è¿›çš„å»é‡é€»è¾‘
"""

import urllib.parse
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from typing import Any

import astrbot.api.message_components as Comp
from astrbot.api import logger
from astrbot.api.event import MessageChain

from ..models.data_source_config import (
    get_intensity_based_sources,
    get_scale_based_sources,
)
from ..models.models import (
    DataSource,
    DisasterEvent,
    EarthquakeData,
    TsunamiData,
    WeatherAlarmData,
)
from ..utils.formatters import (
    BaseMessageFormatter,
    format_earthquake_message,
    format_tsunami_message,
    format_weather_message,
)
from .event_deduplicator import EventDeduplicator
from .filters import (
    EarthquakeKeywordFilter,
    GlobalQuakeFilter,
    IntensityFilter,
    LocalIntensityFilter,
    ReportCountController,
    ScaleFilter,
    USGSFilter,
    WeatherKeywordFilter,
)


class MessagePushManager:
    """æ¶ˆæ¯æ¨é€ç®¡ç†å™¨"""

    def __init__(self, config: dict[str, Any], context):
        self.config = config
        self.context = context

        # åˆå§‹åŒ–è¿‡æ»¤å™¨ - ä½¿ç”¨æ–°çš„é…ç½®è·¯å¾„
        earthquake_filters = config.get("earthquake_filters", {})

        # çƒˆåº¦è¿‡æ»¤å™¨é…ç½®
        intensity_filter_config = earthquake_filters.get("intensity_filter", {})
        self.intensity_filter = IntensityFilter(
            enabled=intensity_filter_config.get("enabled", True),
            min_magnitude=intensity_filter_config.get("min_magnitude", 2.0),
            min_intensity=intensity_filter_config.get("min_intensity", 4.0),
        )

        # éœ‡åº¦è¿‡æ»¤å™¨é…ç½®
        scale_filter_config = earthquake_filters.get("scale_filter", {})
        self.scale_filter = ScaleFilter(
            enabled=scale_filter_config.get("enabled", True),
            min_magnitude=scale_filter_config.get("min_magnitude", 2.0),
            min_scale=scale_filter_config.get("min_scale", 1.0),
        )

        # USGSè¿‡æ»¤å™¨é…ç½®
        magnitude_only_filter_config = earthquake_filters.get(
            "magnitude_only_filter", {}
        )
        self.usgs_filter = USGSFilter(
            enabled=magnitude_only_filter_config.get("enabled", True),
            min_magnitude=magnitude_only_filter_config.get("min_magnitude", 4.5),
        )

        # Global Quakeè¿‡æ»¤å™¨é…ç½®
        global_quake_filter_config = earthquake_filters.get("global_quake_filter", {})
        self.global_quake_filter = GlobalQuakeFilter(
            enabled=global_quake_filter_config.get("enabled", True),
            min_magnitude=global_quake_filter_config.get("min_magnitude", 4.5),
            min_intensity=global_quake_filter_config.get("min_intensity", 5.0),
        )

        # åˆå§‹åŒ–æŠ¥æ•°æ§åˆ¶å™¨
        self.report_controller = ReportCountController(
            push_every_n_reports=config.get("push_frequency_control", {}).get(
                "push_every_n_reports", 3
            ),
            first_report_always_push=config.get("push_frequency_control", {}).get(
                "first_report_always_push", True
            ),
            final_report_always_push=config.get("push_frequency_control", {}).get(
                "final_report_always_push", True
            ),
        )

        # åˆå§‹åŒ–å»é‡å™¨
        self.deduplicator = EventDeduplicator(
            time_window_minutes=config.get("event_deduplication", {}).get(
                "time_window_minutes", 1
            ),
            location_tolerance_km=config.get("event_deduplication", {}).get(
                "location_tolerance_km", 20.0
            ),
            magnitude_tolerance=config.get("event_deduplication", {}).get(
                "magnitude_tolerance", 0.5
            ),
        )

        # äº‹ä»¶æ¨é€è®°å½•
        self.event_push_records: dict[str, list[dict]] = defaultdict(list)

        # ç›®æ ‡ä¼šè¯
        self.target_sessions = self._parse_target_sessions()

        # åˆå§‹åŒ–æœ¬åœ°ç›‘æ§è¿‡æ»¤å™¨
        self.local_monitor = LocalIntensityFilter(config.get("local_monitoring", {}))

        # åˆå§‹åŒ–æ°”è±¡é¢„è­¦å…³é”®è¯è¿‡æ»¤å™¨
        weather_keyword_config = (
            config.get("data_sources", {})
            .get("fan_studio", {})
            .get("weather_keyword_filter", {})
        )
        self.weather_keyword_filter = WeatherKeywordFilter(weather_keyword_config)

        # åˆå§‹åŒ–åœ°éœ‡å…³é”®è¯è¿‡æ»¤å™¨
        earthquake_keyword_config = (
            config.get("earthquake_filters", {}).get("keyword_filter", {})
        )
        self.earthquake_keyword_filter = EarthquakeKeywordFilter(earthquake_keyword_config)

    def _parse_target_sessions(self) -> list[str]:
        """è§£æç›®æ ‡ä¼šè¯ - ä½¿ç”¨æ­£ç¡®çš„é…ç½®é”®å"""
        target_groups = self.config.get("target_groups", [])
        sessions = []

        for group_id in target_groups:
            if group_id:
                # ä½¿ç”¨æ­£ç¡®çš„ä¼šè¯IDæ ¼å¼
                platform_name = self.config.get("platform_name", "aiocqhttp")
                session = f"{platform_name}:GroupMessage:{group_id}"
                sessions.append(session)

        return sessions

    def should_push_event(self, event: DisasterEvent) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¨é€äº‹ä»¶"""
        # 1. æ—¶é—´æ£€æŸ¥ï¼ˆæ‰€æœ‰äº‹ä»¶ç±»å‹ï¼‰- è¿™æ˜¯æœ€é‡è¦çš„è¿‡æ»¤
        # è·å–å¸¦æ—¶åŒºçš„äº‹ä»¶æ—¶é—´
        event_time_aware = self._get_event_time(event)

        if event_time_aware:
            # ä½¿ç”¨UTCå½“å‰æ—¶é—´è¿›è¡Œæ¯”è¾ƒï¼Œç¡®ä¿æ—¶åŒºæ— å…³æ€§
            current_time_utc = datetime.now(timezone.utc)
            time_diff = (
                current_time_utc - event_time_aware
            ).total_seconds() / 3600  # å°æ—¶

            if time_diff > 1:
                logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶æ—¶é—´è¿‡æ—©ï¼ˆ{time_diff:.1f}å°æ—¶å‰ï¼‰ï¼Œè¿‡æ»¤")
                return False

        # 2. éåœ°éœ‡äº‹ä»¶æ£€æŸ¥
        if not isinstance(event.data, EarthquakeData):
            # æ°”è±¡é¢„è­¦äº‹ä»¶éœ€è¦è¿›è¡Œå…³é”®è¯è¿‡æ»¤
            if isinstance(event.data, WeatherAlarmData):
                headline = event.data.headline or event.data.title or ""
                if self.weather_keyword_filter.should_filter(headline):
                    return False
            # æµ·å•¸å’Œæ°”è±¡äº‹ä»¶é€šè¿‡äº†è¿‡æ»¤ï¼Œå¯ä»¥æ¨é€
            return True

        # 3. åœ°éœ‡äº‹ä»¶ä¸“ç”¨è¿‡æ»¤é€»è¾‘
        earthquake = event.data
        source_id = self._get_source_id(event)

        # åœ°éœ‡å…³é”®è¯è¿‡æ»¤ï¼ˆä¼˜å…ˆåº”ç”¨ï¼Œé€‚ç”¨äºæ‰€æœ‰åœ°éœ‡æ•°æ®æºï¼‰
        if self.earthquake_keyword_filter.should_filter(earthquake):
            logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«åœ°éœ‡å…³é”®è¯è¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
            return False

        # æ•°æ®æºä¸“ç”¨è¿‡æ»¤å™¨
        if source_id == "global_quake":
            # Global Quakeä¸“ç”¨è¿‡æ»¤å™¨
            if self.global_quake_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«Global Quakeè¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id in get_intensity_based_sources():
            # ä½¿ç”¨çƒˆåº¦è¿‡æ»¤å™¨
            if self.intensity_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«çƒˆåº¦è¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id in get_scale_based_sources():
            # ä½¿ç”¨éœ‡åº¦è¿‡æ»¤å™¨
            if self.scale_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«éœ‡åº¦è¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False
        elif source_id == "usgs_fanstudio":
            # USGSä¸“ç”¨è¿‡æ»¤å™¨
            if self.usgs_filter.should_filter(earthquake):
                logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«USGSè¿‡æ»¤å™¨è¿‡æ»¤: {source_id}")
                return False

        # æŠ¥æ•°æ§åˆ¶ï¼ˆä»…EEWæ•°æ®æºï¼‰
        if not self.report_controller.should_push_report(event):
            logger.info(f"[ç¾å®³é¢„è­¦] äº‹ä»¶è¢«æŠ¥æ•°æ§åˆ¶å™¨è¿‡æ»¤: {source_id}")
            return False

        # æœ¬åœ°çƒˆåº¦è¿‡æ»¤ä¸æ³¨å…¥ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è¾…åŠ©æ–¹æ³•ï¼‰
        result = self.local_monitor.inject_local_estimation(earthquake)
        # result ä¸º None è¡¨ç¤ºæœªå¯ç”¨ï¼Œå¦åˆ™æ£€æŸ¥ is_allowed
        if result is not None and not result.get("is_allowed", True):
            return False

        return True

    def _get_event_time(self, event: DisasterEvent) -> datetime | None:
        """è·å–ç¾å®³äº‹ä»¶çš„å¸¦æ—¶åŒºæ—¶é—´ (Aware Datetime)"""
        raw_time = None
        if isinstance(event.data, EarthquakeData):
            raw_time = event.data.shock_time
        elif isinstance(event.data, TsunamiData):
            raw_time = event.data.issue_time
        elif isinstance(event.data, WeatherAlarmData):
            raw_time = event.data.effective_time or event.data.issue_time

        if not raw_time:
            return None

        # å¦‚æœå·²ç»æ˜¯Awareæ—¶é—´ï¼Œç›´æ¥è¿”å›
        if raw_time.tzinfo is not None:
            return raw_time

        # æ ¹æ®æ•°æ®æºIDç¡®å®šæ—¶åŒº
        source_id = event.source_id or self._get_source_id(event)

        # å®šä¹‰æ—¶åŒº
        # JST (UTC+9)
        tz_jst = timezone(timedelta(hours=9))
        # CST (UTC+8)
        tz_cst = timezone(timedelta(hours=8))
        # UTC
        tz_utc = timezone.utc

        # 1. UTC+9 æ•°æ®æº
        # - Fan Studio JMA
        # - P2P Quake (æ‰€æœ‰)
        # - Wolfx JMA
        if (
            "jma" in source_id
            or "p2p" in source_id
            or source_id == "wolfx_jma_eew"
            or source_id == "wolfx_jma_eq"
        ):
            return raw_time.replace(tzinfo=tz_jst)

        # 2. UTC æ•°æ®æº
        # - Global Quake
        if "global_quake" in source_id:
            return raw_time.replace(tzinfo=tz_utc)

        # 3. UTC+8 æ•°æ®æº (é»˜è®¤)
        # - Fan Studio (é™¤äº† JMA, USGSå·²è½¬ä¸ºUTC+8)
        # - Wolfx (é™¤äº† JMA)
        # - China Weather/Tsunami
        return raw_time.replace(tzinfo=tz_cst)

    def _get_source_id(self, event: DisasterEvent) -> str:
        """è·å–äº‹ä»¶çš„æ•°æ®æºID"""
        source_mapping = {
            # EEWé¢„è­¦æ•°æ®æº
            DataSource.FAN_STUDIO_CEA.value: "cea_fanstudio",
            DataSource.WOLFX_CENC_EEW.value: "cea_wolfx",
            DataSource.FAN_STUDIO_CWA.value: "cwa_fanstudio",
            DataSource.WOLFX_CWA_EEW.value: "cwa_wolfx",
            DataSource.FAN_STUDIO_JMA.value: "jma_fanstudio",
            DataSource.P2P_EEW.value: "jma_p2p",
            DataSource.WOLFX_JMA_EEW.value: "jma_wolfx",
            # åœ°éœ‡æƒ…æŠ¥æ•°æ®æº
            DataSource.FAN_STUDIO_CENC.value: "cenc_fanstudio",
            DataSource.WOLFX_CENC_EQ.value: "cenc_wolfx",
            DataSource.P2P_EARTHQUAKE.value: "jma_p2p_info",
            DataSource.WOLFX_JMA_EQ.value: "jma_wolfx_info",
            DataSource.FAN_STUDIO_USGS.value: "usgs_fanstudio",
            DataSource.GLOBAL_QUAKE.value: "global_quake",
            # æ°”è±¡å’Œæµ·å•¸é¢„è­¦æ•°æ®æº
            DataSource.FAN_STUDIO_WEATHER.value: "china_weather_fanstudio",
            DataSource.FAN_STUDIO_TSUNAMI.value: "china_tsunami_fanstudio",
            DataSource.P2P_TSUNAMI.value: "jma_tsunami_p2p",
        }

        return source_mapping.get(event.source.value, event.source.value)

    async def push_event(self, event: DisasterEvent) -> bool:
        """æ¨é€äº‹ä»¶"""
        logger.debug(f"[ç¾å®³é¢„è­¦] å¤„ç†äº‹ä»¶æ¨é€: {event.id}")

        # 1. å…ˆå»é‡æ£€æŸ¥ - å…è®¸å¤šæ•°æ®æºæ¨é€åŒä¸€äº‹ä»¶
        if not self.deduplicator.should_push_event(event):
            logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} è¢«å»é‡å™¨è¿‡æ»¤")
            return False

        # 2. æ¨é€æ¡ä»¶æ£€æŸ¥
        if not self.should_push_event(event):
            logger.debug(f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} æœªé€šè¿‡æ¨é€æ¡ä»¶æ£€æŸ¥")
            return False

        try:
            # 3. æ„å»ºæ¶ˆæ¯
            message = self._build_message(event)
            logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯æ„å»ºå®Œæˆ")

            # 4. è·å–ç›®æ ‡ä¼šè¯
            target_sessions = self.target_sessions
            if not target_sessions:
                logger.warning("[ç¾å®³é¢„è­¦] æ²¡æœ‰é…ç½®ç›®æ ‡ä¼šè¯ï¼Œæ— æ³•æ¨é€æ¶ˆæ¯")
                return False

            # 5. æ¨é€æ¶ˆæ¯
            push_success_count = 0
            for session in target_sessions:
                try:
                    await self._send_message(session, message)
                    logger.info(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯å·²æ¨é€åˆ° {session}")
                    push_success_count += 1
                except Exception as e:
                    logger.error(f"[ç¾å®³é¢„è­¦] æ¨é€åˆ° {session} å¤±è´¥: {e}")

            # 6. è®°å½•æ¨é€
            self._record_push(event)
            logger.info(
                f"[ç¾å®³é¢„è­¦] äº‹ä»¶ {event.id} æ¨é€å®Œæˆï¼ŒæˆåŠŸæ¨é€åˆ° {push_success_count} ä¸ªä¼šè¯"
            )
            return push_success_count > 0

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ¨é€äº‹ä»¶å¤±è´¥: {e}")
            return False

    def _build_message(self, event: DisasterEvent) -> MessageChain:
        """æ„å»ºæ¶ˆæ¯ - ä½¿ç”¨æ ¼å¼åŒ–å™¨å¹¶åº”ç”¨æ¶ˆæ¯æ ¼å¼é…ç½®"""
        source_id = self._get_source_id(event)

        # è·å–æ¶ˆæ¯æ ¼å¼é…ç½®
        message_format_config = self.config.get("message_format", {})
        include_map = message_format_config.get("include_map", True)
        map_provider = message_format_config.get("map_provider", "baidu")
        map_zoom_level = message_format_config.get("map_zoom_level", 5)
        detailed_jma = message_format_config.get("detailed_jma_intensity", False)

        logger.debug(
            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯é…ç½®: provider={map_provider}, zoom={map_zoom_level}, detailed_jma={detailed_jma}"
        )

        if isinstance(event.data, WeatherAlarmData):
            message_text = format_weather_message(source_id, event.data)
        elif isinstance(event.data, TsunamiData):
            message_text = format_tsunami_message(source_id, event.data)
        elif isinstance(event.data, EarthquakeData):
            # ä¼ é€’é…ç½®é€‰é¡¹
            options = {"detailed_jma_intensity": detailed_jma}
            message_text = format_earthquake_message(source_id, event.data, options)
        else:
            # æœªçŸ¥äº‹ä»¶ç±»å‹ï¼Œä½¿ç”¨åŸºç¡€æ ¼å¼åŒ–
            logger.warning(f"[ç¾å®³é¢„è­¦] æœªçŸ¥äº‹ä»¶ç±»å‹: {type(event.data)}")
            message_text = f"ğŸš¨[æœªçŸ¥äº‹ä»¶]\nğŸ“‹äº‹ä»¶IDï¼š{event.id}\nâ°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

        # æ„å»ºæ¶ˆæ¯é“¾
        if include_map and isinstance(event.data, EarthquakeData):
            if event.data.latitude is not None and event.data.longitude is not None:
                # ä½¿ç”¨æ¶ˆæ¯æ ¼å¼åŒ–å™¨ä¸­çš„ä¼˜åŒ–åœ°å›¾é“¾æ¥ç”Ÿæˆ
                map_url = BaseMessageFormatter.get_map_link(
                    event.data.latitude,
                    event.data.longitude,
                    map_provider,
                    map_zoom_level,
                    magnitude=event.data.magnitude,
                    place_name=event.data.place_name,
                )
                if map_url:
                    # å…³é”®ä¿®å¤ï¼šç»•è¿‡AstrBotçš„strip()é—®é¢˜
                    # ä½¿ç”¨é›¶å®½ç©ºæ ¼ä¿æŠ¤æ¢è¡Œï¼ŒURLç¼–ç ç¡®ä¿ç‰¹æ®Šå­—ç¬¦å¤„ç†
                    zero_width_space = "\u200b"
                    encoded_map_url = urllib.parse.quote(map_url, safe=":/?&=+")

                    # ç›´æ¥åˆå¹¶åˆ°æ¶ˆæ¯æ–‡æœ¬ä¸­
                    message_text += f"{zero_width_space}\nğŸ—ºï¸åœ°å›¾é“¾æ¥:{zero_width_space} {encoded_map_url}"

        # æ„å»ºæ¶ˆæ¯é“¾
        chain = [Comp.Plain(message_text)]
        return MessageChain(chain)

    def _generate_map_link(
        self, latitude: float, longitude: float, provider: str, zoom: int
    ) -> str:
        """æ ¹æ®é…ç½®ç”Ÿæˆåœ°å›¾é“¾æ¥ - å·²ç§»è‡³message_formattersæ¨¡å—"""
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ç”±message_formattersæ¨¡å—å¤„ç†
        return BaseMessageFormatter.get_map_link(
            latitude,
            longitude,
            provider,
            zoom,
            magnitude=None,  # è¿™ä¸ªæ–¹æ³•æ²¡æœ‰éœ‡çº§ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            place_name=None,  # è¿™ä¸ªæ–¹æ³•æ²¡æœ‰ä½ç½®ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        )

    async def _send_message(self, session: str, message: MessageChain):
        """å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šä¼šè¯"""
        await self.context.send_message(session, message)

    def _record_push(self, event: DisasterEvent):
        """è®°å½•æ¨é€"""
        event_id = self._get_event_id(event)

        # è®°å½•æ¨é€ä¿¡æ¯
        push_info = {
            "timestamp": datetime.now(),
            "event_id": event_id,
            "disaster_type": event.disaster_type.value,
            "source": self._get_source_id(event),
        }

        self.event_push_records[event_id].append(push_info)

    def _get_event_id(self, event: DisasterEvent) -> str:
        """è·å–äº‹ä»¶ID"""
        if isinstance(event.data, EarthquakeData):
            return event.data.event_id or event.data.id
        elif isinstance(event.data, (TsunamiData, WeatherAlarmData)):
            return event.data.id
        return event.id

    def get_push_stats(self) -> dict[str, Any]:
        """è·å–æ¨é€ç»Ÿè®¡"""
        total_events = len(self.event_push_records)
        total_pushes = sum(len(records) for records in self.event_push_records.values())

        return {
            "total_events": total_events,
            "total_pushes": total_pushes,
            "recent_events": self._get_recent_events(),
        }

    def _get_recent_events(self, hours: int = 24) -> list[dict]:
        """è·å–æœ€è¿‘çš„äº‹ä»¶"""
        recent_time = datetime.now() - timedelta(hours=hours)
        recent_events = []

        for event_id, records in self.event_push_records.items():
            recent_records = [
                record for record in records if record["timestamp"] > recent_time
            ]

            if recent_records:
                recent_events.append(
                    {
                        "event_id": event_id,
                        "push_count": len(recent_records),
                        "last_push": max(
                            record["timestamp"] for record in recent_records
                        ),
                    }
                )

        return sorted(recent_events, key=lambda x: x["last_push"], reverse=True)

    def cleanup_old_records(self, days: int = 7):
        """æ¸…ç†æ—§è®°å½•"""
        cutoff_time = datetime.now() - timedelta(days=days)

        # æ¸…ç†äº‹ä»¶æ¨é€è®°å½•
        for event_id in list(self.event_push_records.keys()):
            records = self.event_push_records[event_id]
            recent_records = [
                record for record in records if record["timestamp"] > cutoff_time
            ]

            if recent_records:
                self.event_push_records[event_id] = recent_records
            else:
                del self.event_push_records[event_id]

        # æ¸…ç†å»é‡å™¨
        self.deduplicator.cleanup_old_events()

        logger.info(f"[ç¾å®³é¢„è­¦] å·²æ¸…ç† {days} å¤©å‰çš„æ¨é€è®°å½•")
