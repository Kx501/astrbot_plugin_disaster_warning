"""
åŸå§‹æ¶ˆæ¯è®°å½•å™¨
é€‚é…æ•°æ®æºæ¶æ„ï¼Œæä¾›æ›´å¥½çš„æ—¥å¿—æ ¼å¼å’Œè¿‡æ»¤åŠŸèƒ½
"""

import hashlib
import json
import traceback
from datetime import datetime
from pathlib import Path
from typing import Any

from astrbot.api import logger
from astrbot.api.star import StarTools


class MessageLogger:
    """åŸå§‹æ¶ˆæ¯æ ¼å¼è®°å½•å™¨"""

    def __init__(self, config: dict[str, Any], plugin_name: str):
        self.config = config
        self.plugin_name = plugin_name

        # åŠ è½½P2PåŒºåŸŸä»£ç æ˜ å°„ï¼ˆåŸºäºçœŸå®çš„epsp-area.csvæ–‡ä»¶ï¼‰
        self.p2p_area_mapping = self._load_p2p_area_mapping()

        # åŸºç¡€é…ç½®
        self.enabled = config.get("debug_config", {}).get(
            "enable_raw_message_logging", False
        )
        self.log_file_name = config.get("debug_config", {}).get(
            "raw_message_log_path", "raw_messages.log"
        )
        self.max_size_mb = config.get("debug_config", {}).get("log_max_size_mb", 50)
        self.max_files = config.get("debug_config", {}).get("log_max_files", 5)

        # è¿‡æ»¤é…ç½®
        self.filter_heartbeat = config.get("debug_config", {}).get(
            "filter_heartbeat_messages", True
        )
        self.filter_types = config.get("debug_config", {}).get(
            "filtered_message_types", ["heartbeat", "ping", "pong"]
        )
        self.filter_p2p_areas = config.get("debug_config", {}).get(
            "filter_p2p_areas_messages", True
        )
        self.filter_duplicate_events = config.get("debug_config", {}).get(
            "filter_duplicate_events", True
        )
        self.filter_connection_status = config.get("debug_config", {}).get(
            "filter_connection_status", True
        )

        # ç”¨äºå»é‡çš„ç¼“å­˜
        self.recent_event_hashes: set[str] = set()
        self.recent_raw_logs: list[str] = []  # æ–°å¢ï¼šç”¨äºåŸå§‹æ—¥å¿—æ–‡æœ¬å»é‡
        self.max_cache_size = 1000
        self.max_raw_log_cache = 30  # åªç¼“å­˜æœ€è¿‘30æ¡åŸå§‹æ—¥å¿—ç”¨äºå»é‡

        # æ—¥å¿—è¿‡æ»¤ç»Ÿè®¡
        self.filter_stats = {
            "heartbeat_filtered": 0,
            "p2p_areas_filtered": 0,
            "duplicate_events_filtered": 0,
            "connection_status_filtered": 0,
            "total_filtered": 0,
        }

        # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„ - ä½¿ç”¨AstrBotçš„StarToolsè·å–æ­£ç¡®çš„æ•°æ®ç›®å½•
        self.data_dir = StarTools.get_data_dir("astrbot_plugin_disaster_warning")
        self.log_file_path = self.data_dir / self.log_file_name

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ—¶è¯»å–æ’ä»¶ç‰ˆæœ¬ï¼Œé¿å…æ¯æ¬¡å†™æ—¥å¿—éƒ½è¿›è¡Œæ–‡ä»¶IO
        self.plugin_version = self._get_plugin_version()

        logger.info("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨åˆå§‹åŒ–å®Œæˆ")
        if self.filter_heartbeat:
            logger.info("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è¿‡æ»¤é…ç½®å·²å¯ç”¨:")
            logger.info(f"[ç¾å®³é¢„è­¦] - åŸºç¡€ç±»å‹è¿‡æ»¤: {self.filter_types}")
            logger.info(f"[ç¾å®³é¢„è­¦] - P2PèŠ‚ç‚¹çŠ¶æ€è¿‡æ»¤: {self.filter_p2p_areas}")
            logger.info(f"[ç¾å®³é¢„è­¦] - é‡å¤äº‹ä»¶è¿‡æ»¤: {self.filter_duplicate_events}")
            logger.info(f"[ç¾å®³é¢„è­¦] - è¿æ¥çŠ¶æ€è¿‡æ»¤: {self.filter_connection_status}")

    def _should_filter_message(self, raw_data: Any, source_id: str = "") -> str:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥æ¶ˆæ¯ï¼Œè¿”å›è¿‡æ»¤åŸå› ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸è¿‡æ»¤"""
        if not self.enabled or not self.filter_heartbeat:
            return ""

        try:
            # å¤„ç†ä¸åŒç±»å‹çš„åŸå§‹æ•°æ®
            if isinstance(raw_data, str) and raw_data.strip():
                # å°è¯•è§£æJSONæ•°æ®
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯ä½†ä¸è¿‡æ»¤
                    logger.debug(
                        f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - JSONè§£æå¤±è´¥ï¼Œæ¶ˆæ¯å‰100å­—ç¬¦: {raw_data[:100]}..."
                    )
                    return ""

                # è·å–æ¶ˆæ¯ç±»å‹ç”¨äºè°ƒè¯•
                msg_type = data.get("type", "")
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ£€æŸ¥æ¶ˆæ¯è¿‡æ»¤ï¼Œæ¥æº: {source_id}, ç±»å‹: {msg_type}, æ•°æ®é•¿åº¦: {len(raw_data)}"
                )

                # æ£€æŸ¥æ¶ˆæ¯ç±»å‹
                if msg_type and msg_type.lower() in self.filter_types:
                    self.filter_stats["heartbeat_filtered"] += 1
                    logger.debug(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}")
                    return f"æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}"

                # æ£€æŸ¥P2P areasæ¶ˆæ¯ï¼ˆèŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯ï¼‰
                if self.filter_p2p_areas and self._is_p2p_areas_message(data):
                    self.filter_stats["p2p_areas_filtered"] += 1
                    return "P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥é‡å¤äº‹ä»¶ - æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                if self.filter_duplicate_events:
                    event_hash = self._generate_event_hash(data, source_id)
                    is_duplicate = self._is_duplicate_event(data, source_id)
                    if is_duplicate:
                        self.filter_stats["duplicate_events_filtered"] += 1
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - é‡å¤äº‹ä»¶è¿‡æ»¤ï¼Œå“ˆå¸Œ: {event_hash}, åŸå› : äº‹ä»¶å“ˆå¸Œå·²å­˜åœ¨"
                        )
                        return f"é‡å¤äº‹ä»¶ (å“ˆå¸Œ: {event_hash})"
                    elif event_hash:
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - äº‹ä»¶å“ˆå¸Œç”Ÿæˆ: {event_hash}, å…è®¸è®°å½•"
                        )

                # æ£€æŸ¥è¿æ¥çŠ¶æ€æ¶ˆæ¯
                if self.filter_connection_status and self._is_connection_status_message(
                    data
                ):
                    self.filter_stats["connection_status_filtered"] += 1
                    logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - è¿æ¥çŠ¶æ€æ¶ˆæ¯è¿‡æ»¤")
                    return "è¿æ¥çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥WebSocketæ¶ˆæ¯å†…å®¹ï¼ˆåµŒå¥—JSONï¼‰
                if "raw_data" in data and isinstance(data["raw_data"], str):
                    try:
                        inner_data = json.loads(data["raw_data"])
                        inner_type = inner_data.get("type", "").lower()
                        if inner_type in self.filter_types:
                            self.filter_stats["heartbeat_filtered"] += 1
                            return f"å†…å±‚æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {inner_type}"

                        # æ£€æŸ¥å†…å±‚æ•°æ®çš„P2P areasæ¶ˆæ¯
                        if self.filter_p2p_areas and self._is_p2p_areas_message(
                            inner_data
                        ):
                            self.filter_stats["p2p_areas_filtered"] += 1
                            return "å†…å±‚P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                        # æ£€æŸ¥å†…å±‚æ•°æ®çš„é‡å¤äº‹ä»¶
                        if self.filter_duplicate_events and self._is_duplicate_event(
                            inner_data, source_id
                        ):
                            self.filter_stats["duplicate_events_filtered"] += 1
                            return "å†…å±‚é‡å¤äº‹ä»¶"
                    except (json.JSONDecodeError, AttributeError):
                        pass

            elif isinstance(raw_data, dict):
                # å¦‚æœraw_dataå·²ç»æ˜¯å­—å…¸
                msg_type = raw_data.get("type", "")
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ£€æŸ¥å­—å…¸ç±»å‹æ¶ˆæ¯ï¼Œæ¥æº: {source_id}, ç±»å‹: {msg_type}"
                )

                if msg_type and msg_type.lower() in self.filter_types:
                    self.filter_stats["heartbeat_filtered"] += 1
                    logger.debug(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}")
                    return f"æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}"

                # æ£€æŸ¥P2P areasæ¶ˆæ¯
                if self.filter_p2p_areas and self._is_p2p_areas_message(raw_data):
                    self.filter_stats["p2p_areas_filtered"] += 1
                    return "P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥é‡å¤äº‹ä»¶ - æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                if self.filter_duplicate_events:
                    event_hash = self._generate_event_hash(raw_data, source_id)
                    is_duplicate = self._is_duplicate_event(raw_data, source_id)
                    if is_duplicate:
                        self.filter_stats["duplicate_events_filtered"] += 1
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - é‡å¤äº‹ä»¶è¿‡æ»¤ï¼Œå“ˆå¸Œ: {event_hash}"
                        )
                        return f"é‡å¤äº‹ä»¶ (å“ˆå¸Œ: {event_hash})"

                # æ£€æŸ¥è¿æ¥çŠ¶æ€æ¶ˆæ¯
                if self.filter_connection_status and self._is_connection_status_message(
                    raw_data
                ):
                    self.filter_stats["connection_status_filtered"] += 1
                    return "è¿æ¥çŠ¶æ€æ¶ˆæ¯"

        except (json.JSONDecodeError, KeyError, TypeError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œä¸è¿‡æ»¤
            pass

        return ""

    def _is_p2p_areas_message(self, data: dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºP2P areasæ¶ˆæ¯ï¼ˆèŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯ï¼‰"""
        if "areas" in data and isinstance(data["areas"], list):
            areas = data["areas"]
            if areas and all(
                isinstance(area, dict) and "peer" in area for area in areas[:3]
            ):
                return True
        return False

    def _is_duplicate_event(self, data: dict[str, Any], source_id: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤äº‹ä»¶"""
        try:
            event_hash = self._generate_event_hash(data, source_id)
            if event_hash in self.recent_event_hashes:
                return True

            # æ·»åŠ åˆ°ç¼“å­˜ï¼ˆLRUé£æ ¼ï¼‰
            if len(self.recent_event_hashes) >= self.max_cache_size:
                # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•å®ç°ï¼‰
                oldest = next(iter(self.recent_event_hashes))
                self.recent_event_hashes.remove(oldest)

            self.recent_event_hashes.add(event_hash)
            return False

        except Exception as e:
            logger.debug(f"[ç¾å®³é¢„è­¦] å»é‡æ£€æŸ¥å¼‚å¸¸: {e}")
            return False

    def _extract_payload(self, data: dict[str, Any]) -> dict[str, Any]:
        """æå–å®é™…æ•°æ®è½½è· - å…¼å®¹å¤šå±‚åµŒå¥—ç»“æ„"""
        if not isinstance(data, dict):
            return {}

        # 1. ä¼˜å…ˆæ£€æŸ¥ FAN Studio é£æ ¼çš„ Data/data
        if "Data" in data and isinstance(data["Data"], dict):
            return data["Data"]
        elif "data" in data and isinstance(data["data"], dict):
            return data["data"]

        # 2. æ£€æŸ¥ P2P Quake é£æ ¼ (ç›´æ¥åœ¨æ ¹èŠ‚ç‚¹ï¼Œä½†æœ‰ code/issue)
        if "code" in data and "issue" in data:
            return data

        # 3. æ£€æŸ¥ Wolfx é£æ ¼ (æ‰å¹³ç»“æ„)
        if "type" in data and ("EventID" in data or "ID" in data):
            return data

        # 4. é»˜è®¤è¿”å›åŸæ•°æ®
        return data

    def _generate_event_hash(self, data: dict[str, Any], source_id: str) -> str:
        """ç”Ÿæˆäº‹ä»¶å“ˆå¸Œç”¨äºå»é‡ - æ™ºèƒ½è¯†åˆ«äº‹ä»¶ç±»å‹"""
        # æå–å®é™…è½½è·
        payload = self._extract_payload(data)

        # åŸºäºäº‹ä»¶çš„å…³é”®å­—æ®µç”Ÿæˆå“ˆå¸Œ
        hash_parts = [f"source:{source_id}"]

        # é¦–å…ˆè¿›è¡Œäº‹ä»¶ç±»å‹æ™ºèƒ½è¯†åˆ«
        event_type = self._detect_event_type(data, payload)
        hash_parts.append(f"etype:{event_type}")

        # ä¸åŒç±»å‹çš„äº‹ä»¶ä½¿ç”¨ä¸åŒçš„å»é‡ç­–ç•¥
        if event_type == "weather":
            return self._generate_weather_hash(payload, hash_parts)
        elif event_type == "earthquake":
            return self._generate_earthquake_hash(payload, hash_parts)
        elif event_type == "tsunami":
            return self._generate_tsunami_hash(payload, hash_parts)
        else:
            return self._generate_generic_hash(payload, hash_parts)

    def _detect_event_type(self, data: dict[str, Any], payload: dict[str, Any]) -> str:
        """æ™ºèƒ½æ£€æµ‹äº‹ä»¶ç±»å‹"""
        # æ£€æŸ¥æ¶ˆæ¯ç±»å‹å­—æ®µ (ä¼˜å…ˆæ£€æŸ¥å¤–å±‚ï¼Œå†æ£€æŸ¥å†…å±‚)
        msg_type = str(data.get("type", "")).lower()
        if not msg_type:
            msg_type = str(payload.get("type", "")).lower()

        # ä½¿ç”¨msg_typeè¿›è¡Œäº‹ä»¶ç±»å‹åˆ¤æ–­
        if msg_type in ["weather", "alarm", "warning"]:
            return "weather"
        # ç§»é™¤ eqlistï¼Œè®©å…¶å›é€€åˆ° generic ä½¿ç”¨ MD5 å“ˆå¸Œï¼Œç¡®ä¿åˆ—è¡¨æ›´æ–°èƒ½è¢«æ£€æµ‹åˆ°
        elif msg_type in ["earthquake", "seismic", "jma_eew", "cenc_eew", "cwa_eew"]:
            return "earthquake"
        elif msg_type in ["tsunami"]:
            return "tsunami"

        # æ£€æŸ¥æ•°æ®å†…å®¹ç‰¹å¾
        data_str = str(data).lower() + str(payload).lower()

        # æ°”è±¡é¢„è­¦ç‰¹å¾
        if any(
            k in data_str for k in ["weather", "alarm", "é¢„è­¦", "warning", "headline"]
        ):
            if not any(
                k in data_str for k in ["åœ°éœ‡", "earthquake", "magnitude", "éœ‡çº§"]
            ):
                return "weather"

        # åœ°éœ‡äº‹ä»¶ç‰¹å¾
        if any(
            k in data_str
            for k in ["earthquake", "åœ°éœ‡", "magnitude", "éœ‡çº§", "hypocenter", "éœ‡æº"]
        ):
            return "earthquake"

        # æµ·å•¸é¢„è­¦ç‰¹å¾
        if any(k in data_str for k in ["tsunami", "æµ·å•¸", "æ´¥æ³¢"]):
            return "tsunami"

        # P2Påœ°éœ‡ä¿¡æ¯ (æ£€æŸ¥ payload)
        if "code" in payload and isinstance(payload.get("code"), int):
            code = payload["code"]
            if code in [551, 556]:
                return "earthquake"
            if code in [552]:
                return "tsunami"

        return "generic"

    def _generate_weather_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆæ°”è±¡é¢„è­¦å“ˆå¸Œ"""
        # 1. å°è¯•è·å–å”¯ä¸€ID
        event_id = data.get("id") or data.get("alertId") or data.get("identifier")
        if event_id:
            hash_parts.append(f"wid:{event_id}")
            return "|".join(hash_parts)

        # 2. ç»„åˆå…³é”®å­—æ®µä½œä¸ºID
        # æ ‡é¢˜/Headline
        headline = data.get("headline") or data.get("title") or ""
        if headline:
            hash_parts.append(f"wh:{headline[:30]}")

        # åœ°åŒº/Area
        area = data.get("areaDesc") or data.get("sender") or ""
        if area:
            hash_parts.append(f"wa:{area}")

        # æ—¶é—´/Time (ç²¾ç¡®åˆ°åˆ†é’Ÿ)
        time_info = (
            data.get("effective")
            or data.get("issue_time")
            or data.get("time")
            or data.get("sendTime")
        )
        if time_info:
            hash_parts.append(f"wt:{str(time_info)[:16]}")

        return "|".join(hash_parts)

    def _generate_earthquake_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆåœ°éœ‡äº‹ä»¶å“ˆå¸Œ"""
        # 1. å°è¯•è·å–äº‹ä»¶ID
        event_id = (
            data.get("id")
            or data.get("eventId")
            or data.get("EventID")
            or data.get("md5")
        )
        if event_id:
            hash_parts.append(f"eq_id:{event_id}")

            # é’ˆå¯¹EEWï¼Œå¿…é¡»é™„åŠ æŠ¥æ•°ä¿¡æ¯
            report_num = (
                data.get("updates")
                or data.get("ReportNum")
                or data.get("serial")
                or data.get("issue", {}).get("serial")
            )
            if report_num:
                hash_parts.append(f"rn:{report_num}")

            # é™„åŠ æœ€ç»ˆæŠ¥æ ‡å¿—
            if data.get("isFinal") or data.get("is_final"):
                hash_parts.append("final")

            # é™„åŠ ä¿¡æ¯ç±»å‹ï¼ˆè‡ªåŠ¨/æ­£å¼ï¼‰ï¼Œç¡®ä¿çŠ¶æ€å˜æ›´æ—¶ç”Ÿæˆæ–°å“ˆå¸Œ
            info_type = data.get("infoTypeName") or data.get("type")
            if info_type:
                hash_parts.append(f"it:{info_type}")

            # é’ˆå¯¹æ— æŠ¥æ•°æœºåˆ¶çš„æ•°æ®æºï¼ˆå¦‚USGSï¼‰ï¼ŒåŠ å…¥æ›´æ–°æ—¶é—´æˆ–éœ‡çº§ä»¥åŒºåˆ†ä¿®æ­£
            if not report_num:
                # å°è¯•è·å–æ›´æ–°æ—¶é—´
                updated = data.get("updated") or data.get("updateTime")
                if updated:
                    hash_parts.append(f"up:{str(updated)}")

                # å°è¯•è·å–éœ‡çº§ï¼ˆä¿ç•™1ä½å°æ•°ï¼‰ï¼Œç¡®ä¿éœ‡çº§ä¿®æ­£èƒ½è¢«è®°å½•
                mag = data.get("magnitude") or data.get("Magnitude")
                if mag:
                    hash_parts.append(f"m:{mag}")

            return "|".join(hash_parts)

        # 2. å¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨ç‰¹å¾ç»„åˆ
        # æ—¶é—´ (ç²¾ç¡®åˆ°åˆ†é’Ÿ)
        time_info = data.get("shockTime") or data.get("time") or data.get("OriginTime")
        if time_info:
            hash_parts.append(f"et:{str(time_info)[:16]}")

        # éœ‡çº§
        mag = data.get("magnitude") or data.get("Magnitude")
        if mag:
            hash_parts.append(f"em:{mag}")

        # ä½ç½® (ä¿ç•™1ä½å°æ•°)
        lat = data.get("latitude") or data.get("Latitude")
        lon = data.get("longitude") or data.get("Longitude")
        if lat and lon:
            try:
                hash_parts.append(f"el:{float(lat):.1f},{float(lon):.1f}")
            except (ValueError, TypeError):
                pass

        return "|".join(hash_parts)

    def _generate_tsunami_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆæµ·å•¸é¢„è­¦å“ˆå¸Œ"""
        # 1. å°è¯•è·å–ID
        event_id = data.get("id") or data.get("code")
        if event_id:
            hash_parts.append(f"tid:{event_id}")

            # é™„åŠ æ›´æ–°æ—¶é—´æˆ–æŠ¥æ•°
            time_info = data.get("issue_time") or data.get("time")
            if time_info:
                hash_parts.append(f"tt:{str(time_info)[:16]}")

            return "|".join(hash_parts)

        # 2. ç‰¹å¾ç»„åˆ
        title = data.get("title") or ""
        if title:
            hash_parts.append(f"tt:{title}")

        time_info = data.get("issue_time") or data.get("time") or data.get("effective")
        if time_info:
            hash_parts.append(f"tm:{str(time_info)[:16]}")

        return "|".join(hash_parts)

    def _generate_generic_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆé€šç”¨å“ˆå¸Œ"""
        # å°è¯•æ‰€æœ‰å¯èƒ½çš„IDå­—æ®µ
        for key in ["id", "ID", "eventId", "EventID", "code", "md5"]:
            if val := data.get(key):
                hash_parts.append(f"gid:{val}")
                return "|".join(hash_parts)

        # å¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨å†…å®¹å“ˆå¸Œï¼ˆå–å‰50ä¸ªå­—ç¬¦ï¼‰
        content_hash = hashlib.md5(str(data).encode()).hexdigest()[:8]
        hash_parts.append(f"gh:{content_hash}")

        return "|".join(hash_parts)

    def _parse_datetime_for_hash(self, time_str: str) -> datetime | None:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ç”¨äºå“ˆå¸Œç”Ÿæˆ"""
        if not time_str:
            return None

        # å°è¯•å¤šç§æ ¼å¼
        formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%Y/%m/%d %H:%M",
            "%Y-%m-%d",
            "%Y/%m/%d",
        ]

        for fmt in formats:
            try:
                return datetime.strptime(time_str.strip(), fmt)
            except ValueError:
                continue

        return None

    def _is_connection_status_message(self, data: dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¿æ¥çŠ¶æ€æ¶ˆæ¯"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¿æ¥å»ºç«‹ã€æ–­å¼€ç­‰çŠ¶æ€æ¶ˆæ¯
        msg_type = data.get("type", "").lower()
        if msg_type in ["connect", "disconnect", "connection", "status"]:
            return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿æ¥ç›¸å…³çš„å…³é”®è¯
        connection_keywords = [
            "connected",
            "disconnected",
            "connection",
            "status",
            "online",
            "offline",
        ]
        message_str = str(data).lower()
        if any(keyword in message_str for keyword in connection_keywords):
            # è¿›ä¸€æ­¥æ£€æŸ¥ï¼Œç¡®ä¿ä¸æ˜¯å®é™…çš„ç¾å®³äº‹ä»¶
            disaster_keywords = [
                "earthquake",
                "åœ°éœ‡",
                "éœ‡çº§",
                "magnitude",
                "tsunami",
                "æµ·å•¸",
                "weather",
                "æ°”è±¡",
            ]
            if not any(keyword in message_str for keyword in disaster_keywords):
                return True

        return False

    def _format_readable_log(self, log_entry: dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¯è¯»æ€§å¼ºçš„æ—¥å¿—å†…å®¹"""
        try:
            # åŸºç¡€ä¿¡æ¯æ ¼å¼åŒ–
            timestamp = datetime.fromisoformat(log_entry["timestamp"]).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            source = log_entry["source"]
            message_type = log_entry["message_type"]

            # æ„å»ºå¯è¯»æ€§å¼ºçš„æ—¥å¿—å¤´éƒ¨
            log_content = f"\n{'=' * 35}\n"
            log_content += f"ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´: {timestamp}\n"
            log_content += f"ğŸ“¡ æ¥æº: {source}\n"
            log_content += f"ğŸ“‹ ç±»å‹: {message_type}\n"

            # æ·»åŠ è¿æ¥ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            connection_info = log_entry.get("connection_info", {})
            if connection_info:
                log_content += "ğŸ”— è¿æ¥: "
                if "url" in connection_info:
                    log_content += f"URL: {connection_info['url']}"
                elif "server" in connection_info and "port" in connection_info:
                    log_content += (
                        f"æœåŠ¡å™¨: {connection_info['server']}:{connection_info['port']}"
                    )
                log_content += "\n"

            # æ ¼å¼åŒ–åŸå§‹æ•°æ®
            raw_data = log_entry["raw_data"]
            log_content += "\nğŸ“Š åŸå§‹æ•°æ®:\n"

            # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œä¸åŒçš„æ ¼å¼åŒ–
            if isinstance(raw_data, str):
                # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                try:
                    parsed_data = json.loads(raw_data)
                    log_content += self._format_json_data(parsed_data, indent=2)
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æ˜¾ç¤º
                    log_content += f"  {raw_data}\n"
            elif isinstance(raw_data, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                log_content += self._format_json_data(raw_data, indent=2)
            else:
                # å…¶ä»–æ ¼å¼
                log_content += f"  {str(raw_data)}\n"

            # æ·»åŠ æ’ä»¶ä¿¡æ¯
            log_content += (
                f"\nğŸ”§ æ’ä»¶ç‰ˆæœ¬: {log_entry.get('plugin_version', 'unknown')}\n"
            )
            log_content += f"{'=' * 35}\n"

            return log_content

        except Exception as e:
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„JSONæ ¼å¼
            logger.warning(f"[ç¾å®³é¢„è­¦] æ—¥å¿—æ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ ¼å¼: {e}")
            return json.dumps(log_entry, ensure_ascii=False, indent=2) + "\n\n"

    def _format_json_data(self, data: dict[str, Any], indent: int = 0) -> str:
        """é€’å½’æ ¼å¼åŒ–JSONæ•°æ®ï¼Œå¢åŠ å¯è¯»æ€§"""
        result = ""
        indent_str = "  " * indent

        for key, value in data.items():
            # é”®åç¿»è¯‘å’Œæ ¼å¼åŒ–
            key_display = self._get_display_key(key)

            if isinstance(value, dict):
                result += f"{indent_str}ğŸ“‹ {key_display}:\n"
                result += self._format_json_data(value, indent + 1)
            elif isinstance(value, list):
                if len(value) > 0:
                    result += f"{indent_str}ğŸ“‹ {key_display} ({len(value)}é¡¹):\n"
                    for i, item in enumerate(value[:5]):  # åªæ˜¾ç¤ºå‰5é¡¹
                        if isinstance(item, dict):
                            result += f"{indent_str}  [{i + 1}]:\n"
                            result += self._format_json_data(item, indent + 2)
                        else:
                            result += f"{indent_str}  [{i + 1}]: {item}\n"
                    if len(value) > 5:
                        result += f"{indent_str}  ... è¿˜æœ‰ {len(value) - 5} é¡¹\n"
                else:
                    result += f"{indent_str}ğŸ“‹ {key_display}: []\n"
            else:
                # æ ¼å¼åŒ–å…·ä½“å€¼
                value_display = self._format_value(key, value)
                result += f"{indent_str}ğŸ“‹ {key_display}: {value_display}\n"

        return result

    def _get_display_key(self, key: str) -> str:
        """è·å–æ ¼å¼åŒ–çš„é”®åæ˜¾ç¤º - æ•´ç†åˆ†ç±»ï¼Œå»é™¤é‡å¤"""
        key_mappings = {
            # ğŸŒ åŸºç¡€ä¿¡æ¯å­—æ®µ (æ‰€æœ‰æ•°æ®æºé€šç”¨)
            "id": "ID",
            "_id": "æ•°æ®åº“ID",
            "type": "æ¶ˆæ¯ç±»å‹",
            "title": "æ ‡é¢˜",
            "code": "æ¶ˆæ¯ä»£ç ",
            "source": "æ•°æ®æ¥æº",
            "status": "çŠ¶æ€",
            "time": "å‘ç”Ÿæ—¶é—´",
            "createTime": "åˆ›å»ºæ—¶é—´",
            "updateTime": "æ›´æ–°æ—¶é—´",
            # ğŸ”ï¸ åœ°éœ‡æ ¸å¿ƒä¿¡æ¯
            "earthquake": "åœ°éœ‡ä¿¡æ¯",
            "magnitude": "éœ‡çº§",
            "Magunitude": "éœ‡çº§",  # Wolfxæ‹¼å†™
            "depth": "æ·±åº¦(km)",
            "Depth": "æ·±åº¦(km)",  # å¤§å†™ç‰ˆæœ¬
            "latitude": "çº¬åº¦",
            "Latitude": "çº¬åº¦",  # å¤§å†™ç‰ˆæœ¬
            "longitude": "ç»åº¦",
            "Longitude": "ç»åº¦",  # å¤§å†™ç‰ˆæœ¬
            "placeName": "åœ°å",
            "name": "åœ°ç‚¹åç§°",
            "shockTime": "å‘éœ‡æ—¶é—´",
            "OriginTime": "å‘éœ‡æ—¶é—´",  # JMAæ ¼å¼
            "hypocenter": "éœ‡æºä¿¡æ¯",
            "Hypocenter": "éœ‡æºåœ°å",  # JMAæ ¼å¼
            # ğŸ“ éœ‡åº¦/çƒˆåº¦ä¿¡æ¯
            "maxScale": "æœ€å¤§éœ‡åº¦(åŸå§‹)",
            "MaxIntensity": "æœ€å¤§éœ‡åº¦",  # JMA/Wolfxæ ¼å¼
            "maxIntensity": "æœ€å¤§çƒˆåº¦",  # Wolfxæ ¼å¼
            "epiIntensity": "é¢„ä¼°çƒˆåº¦",  # FAN Studioæ ¼å¼
            "intensity": "çƒˆåº¦",
            "scale": "éœ‡åº¦å€¼",  # P2Pæ ¼å¼
            # ğŸŒŠ æµ·å•¸ç›¸å…³ä¿¡æ¯
            "domesticTsunami": "æ—¥æœ¬å¢ƒå†…æµ·å•¸",
            "foreignTsunami": "æµ·å¤–æµ·å•¸",
            "tsunami": "æµ·å•¸ä¿¡æ¯",
            "info": "æµ·å•¸ä¿¡æ¯",  # Wolfxæ ¼å¼
            # ğŸ“‹ äº‹ä»¶æ ‡è¯†ä¿¡æ¯
            "eventId": "äº‹ä»¶ID",
            "EventID": "äº‹ä»¶ID",  # JMAæ ¼å¼
            "event_id": "äº‹ä»¶ID",  # ä¸‹åˆ’çº¿ç‰ˆæœ¬
            "EventId": "äº‹ä»¶ç¼–ç ",  # FAN Studioæ ¼å¼
            "Serial": "æŠ¥åºå·",  # JMAæ ¼å¼
            "updates": "æ›´æ–°æ¬¡æ•°",
            "ReportNum": "å‘æŠ¥æ•°",  # Wolfxæ ¼å¼
            # â° æ—¶é—´ç›¸å…³
            "AnnouncedTime": "å‘å¸ƒæ—¶é—´",  # JMAæ ¼å¼
            "ReportTime": "å‘æŠ¥æ—¶é—´",  # Wolfxæ ¼å¼
            "effective": "ç”Ÿæ•ˆæ—¶é—´",  # FAN Studioæ ¼å¼
            "issue_time": "å‘å¸ƒæ—¶é—´",
            "arrivalTime": "åˆ°è¾¾æ—¶é—´",  # æµ·å•¸
            # ğŸ¯ çŠ¶æ€æ ‡å¿—
            "isFinal": "æœ€ç»ˆæŠ¥",
            "isCancel": "å–æ¶ˆæŠ¥",
            "is_final": "æœ€ç»ˆæŠ¥",
            "is_cancel": "å–æ¶ˆæŠ¥",
            "cancelled": "å–æ¶ˆæ ‡å¿—",  # P2Pæ ¼å¼
            "is_training": "è®­ç»ƒæ¨¡å¼",
            "isTraining": "è®­ç»ƒæŠ¥",  # Wolfxæ ¼å¼
            "isSea": "æµ·åŸŸåœ°éœ‡",  # Wolfxæ ¼å¼
            "isAssumption": "æ¨å®šéœ‡æº",  # Wolfxæ ¼å¼
            "isWarn": "è­¦æŠ¥æ ‡å¿—",  # Wolfxæ ¼å¼
            "immediate": "ç´§æ€¥æ ‡å¿—",  # æµ·å•¸
            # ğŸ“° å†…å®¹æè¿°
            "headline": "é¢„è­¦æ ‡é¢˜",  # FAN Studioæ ¼å¼
            "description": "è¯¦ç»†æè¿°",  # FAN Studioæ ¼å¼
            "infoTypeName": "ä¿¡æ¯ç±»å‹",  # FAN Studioæ ¼å¼
            "correct": "è®¢æ­£ä¿¡æ¯",
            "issue": "å‘å¸ƒä¿¡æ¯",
            # ğŸ—ºï¸ åœ°ç†åŒºåŸŸ
            "province": "çœä»½",  # FAN Studioæ ¼å¼
            "pref": "éƒ½é“åºœå¿",  # P2Pæ ¼å¼
            "addr": "è§‚æµ‹ç‚¹åœ°å€",  # P2Pæ ¼å¼
            "location": "éœ‡æºåœ°",  # Wolfxæ ¼å¼
            "area": "åŒºåŸŸä»£ç ",  # P2Pæ ¼å¼
            "isArea": "åŒºåŸŸæ ‡å¿—",  # P2Pæ ¼å¼
            # ğŸ”— é“¾æ¥å’Œå‚è€ƒ
            "url": "å®˜æ–¹é“¾æ¥",
            "OriginalText": "åŸç”µæ–‡",  # Wolfxæ ¼å¼
            # ğŸ“Š ç²¾åº¦å’Œå¯ä¿¡åº¦
            "Accuracy.Epicenter": "éœ‡ä¸­ç²¾åº¦",  # Wolfxæ ¼å¼
            "Accuracy.Depth": "æ·±åº¦ç²¾åº¦",  # Wolfxæ ¼å¼
            "Accuracy.Magnitude": "éœ‡çº§ç²¾åº¦",  # Wolfxæ ¼å¼
            "confidence": "å¯ä¿¡åº¦",  # P2Pæ ¼å¼
            # ğŸŒŠ æµ·å•¸è¯¦ç»†ä¿¡æ¯
            "warningInfo": "è­¦æŠ¥æ ¸å¿ƒä¿¡æ¯",
            "timeInfo": "æ—¶é—´ä¿¡æ¯",
            "details": "è¯¦ç»†ä¿¡æ¯",
            "forecasts": "æ²¿æµ·é¢„æŠ¥",
            "waterLevelMonitoring": "æ°´ä½ç›‘æµ‹",
            "estimatedArrivalTime": "é¢„è®¡åˆ°è¾¾æ—¶é—´",
            "maxWaveHeight": "æœ€å¤§æ³¢é«˜",
            "warningLevel": "è­¦æŠ¥çº§åˆ«",
            "stationName": "ç›‘æµ‹ç«™åç§°",
            "firstHeight": "åˆæ³¢ä¿¡æ¯",  # æµ·å•¸
            "maxHeight": "æœ€å¤§æ³¢é«˜",  # æµ·å•¸
            "condition": "çŠ¶æ€æè¿°",  # æµ·å•¸
            "grade": "é¢„è­¦çº§åˆ«",  # æµ·å•¸
            # ğŸ“ è§‚æµ‹ç‚¹ä¿¡æ¯ (P2P)
            "points": "éœ‡åº¦è§‚æµ‹ç‚¹",
            "comments": "é™„åŠ è¯„è®º",
            "freeFormComment": "è‡ªç”±é™„åŠ æ–‡",
            "areas": "é¢„è­¦åŒºåŸŸ",  # æµ·å•¸å’ŒP2P
            # âš ï¸ å˜æ›´å’Œè­¦æŠ¥ä¿¡æ¯
            "MaxIntChange.String": "éœ‡åº¦å˜æ›´è¯´æ˜",  # Wolfxæ ¼å¼
            "MaxIntChange.Reason": "éœ‡åº¦å˜æ›´åŸå› ",  # Wolfxæ ¼å¼
            "CodeType": "å‘æŠ¥è¯´æ˜",  # Wolfxæ ¼å¼
            "Title": "å‘æŠ¥æŠ¥å¤´",  # Wolfxæ ¼å¼
            # ğŸ”§ æŠ€æœ¯å­—æ®µ
            "autoFlag": "è‡ªåŠ¨æ ‡å¿—",  # FAN Studioæ ¼å¼
            "earthtype": "åœ°éœ‡ç±»å‹",  # FAN Studioæ ¼å¼
            "md5": "æ ¡éªŒç ",
            # ğŸ”Œ è¿æ¥ä¿¡æ¯ (ä¿ç•™åŸæœ‰)
            "connection_type": "è¿æ¥ç±»å‹",
            "server": "æœåŠ¡å™¨",
            "port": "ç«¯å£",
            "status_code": "çŠ¶æ€ç ",
        }

        return key_mappings.get(key, key)

    def _format_value(self, key: str, value: Any) -> str:
        """æ ¼å¼åŒ–å…·ä½“å€¼"""
        if value is None:
            return "æ— æ•°æ®"
        elif value == "":
            return "ç©ºå­—ç¬¦ä¸²"
        elif isinstance(value, (int, float)):
            # ç‰¹æ®Šæ•°å€¼æ ¼å¼åŒ–
            if key == "maxScale" and isinstance(value, int):
                scale_map = {
                    10: "éœ‡åº¦1",
                    20: "éœ‡åº¦2",
                    30: "éœ‡åº¦3",
                    40: "éœ‡åº¦4",
                    45: "éœ‡åº¦5å¼±",
                    50: "éœ‡åº¦5å¼·",
                    55: "éœ‡åº¦6å¼±",
                    60: "éœ‡åº¦6å¼·",
                    70: "éœ‡åº¦7",
                }
                return f"{value} ({scale_map.get(value, 'æœªçŸ¥')})"
            elif key in ["magnitude", "Magnitude", "Magunitude"] and isinstance(
                value, (int, float)
            ):
                return f"M{value}"
            elif key in ["depth", "Depth"] and isinstance(value, (int, float)):
                return f"{value}km"
            elif key == "area" and isinstance(value, int):
                # P2Påœ°éœ‡æ„ŸçŸ¥ä¿¡æ¯çš„åŒºåŸŸä»£ç  - ä½¿ç”¨çœŸå®çš„CSVæ•°æ®
                region_name = self.p2p_area_mapping.get(value, f"åŒºåŸŸä»£ç {value}")
                return f"{value} ({region_name})"
            else:
                return str(value)
        elif isinstance(value, str):
            # å­—ç¬¦ä¸²é•¿åº¦æ§åˆ¶
            if len(value) > 50:
                return f"{value[:47]}..."
            return value
        else:
            return str(value)

    def _load_p2p_area_mapping(self) -> dict[int, str]:
        """åŠ è½½P2PåŒºåŸŸä»£ç æ˜ å°„ï¼ˆåŸºäºçœŸå®çš„epsp-area.csvæ–‡ä»¶ï¼‰"""
        area_mapping = {}

        try:
            # è¯»å–çœŸå®çš„åŒºåŸŸä»£ç æ–‡ä»¶
            csv_path = Path(__file__).parent.parent / "resources/epsp-area.csv"
            if csv_path.exists():
                with open(csv_path, encoding="utf-8") as f:
                    # è·³è¿‡æ ‡é¢˜è¡Œ
                    next(f)

                    for line in f:
                        parts = line.strip().split(",")
                        if len(parts) >= 5:
                            try:
                                # è·å–æ•°å€¼å‹åŒºåŸŸä»£ç å’Œåœ°åŸŸåç§°
                                area_code = int(parts[1])  # åœ°åŸŸã‚³ãƒ¼ãƒ‰(æ•°å€¤å‹)
                                region_name = parts[4]  # åœ°åŸŸ

                                if area_code and region_name:
                                    area_mapping[area_code] = region_name
                            except (ValueError, IndexError):
                                continue

                logger.info(
                    f"[ç¾å®³é¢„è­¦] æˆåŠŸåŠ è½½ {len(area_mapping)} ä¸ªP2PåŒºåŸŸä»£ç æ˜ å°„"
                )
            else:
                logger.warning("[ç¾å®³é¢„è­¦] æœªæ‰¾åˆ°epsp-area.csvæ–‡ä»¶ï¼Œä½¿ç”¨å¤‡ç”¨æ˜ å°„")
                area_mapping = self._get_fallback_area_mapping()

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] åŠ è½½P2PåŒºåŸŸä»£ç æ˜ å°„å¤±è´¥: {e}")
            logger.error("[ç¾å®³é¢„è­¦] è¯·æ£€æŸ¥epsp-area.csvæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
            area_mapping = self._get_fallback_area_mapping()

        return area_mapping

    def _get_fallback_area_mapping(self) -> dict[int, str]:
        """å¤‡ç”¨åŒºåŸŸä»£ç æ˜ å°„ï¼ˆåŸºäºCSVæ–‡ä»¶çš„ä¸»è¦åŒºåŸŸï¼‰"""
        return {
            # ä¸»è¦åŒºåŸŸä»£ç ï¼ˆä»CSVä¸­æå–çš„æœ€å¸¸ç”¨ä»£ç ï¼‰
            10: "åŒ—æµ·é“ çŸ³ç‹©",
            15: "åŒ—æµ·é“ æ¸¡å³¶",
            20: "åŒ—æµ·é“ æªœå±±",
            25: "åŒ—æµ·é“ å¾Œå¿—",
            30: "åŒ—æµ·é“ ç©ºçŸ¥",
            35: "åŒ—æµ·é“ ä¸Šå·",
            40: "åŒ—æµ·é“ ç•™èŒ",
            45: "åŒ—æµ·é“ å®—è°·",
            50: "åŒ—æµ·é“ ç¶²èµ°",
            55: "åŒ—æµ·é“ èƒ†æŒ¯",
            60: "åŒ—æµ·é“ æ—¥é«˜",
            65: "åŒ—æµ·é“ åå‹",
            70: "åŒ—æµ·é“ é‡§è·¯",
            75: "åŒ—æµ·é“ æ ¹å®¤",
            100: "é’æ£®æ´¥è»½",
            105: "é’æ£®ä¸‰å…«ä¸ŠåŒ—",
            106: "é’æ£®ä¸‹åŒ—",
            110: "å²©æ‰‹æ²¿å²¸åŒ—éƒ¨",
            111: "å²©æ‰‹æ²¿å²¸å—éƒ¨",
            115: "å²©æ‰‹å†…é™¸",
            120: "å®®åŸåŒ—éƒ¨",
            125: "å®®åŸå—éƒ¨",
            130: "ç§‹ç”°æ²¿å²¸",
            135: "ç§‹ç”°å†…é™¸",
            140: "å±±å½¢åº„å†…",
            141: "å±±å½¢æœ€ä¸Š",
            142: "å±±å½¢æ‘å±±",
            143: "å±±å½¢ç½®è³œ",
            150: "ç¦å³¶ä¸­é€šã‚Š",
            151: "ç¦å³¶æµœé€šã‚Š",
            152: "ç¦å³¶ä¼šæ´¥",
            200: "èŒ¨åŸåŒ—éƒ¨",
            205: "èŒ¨åŸå—éƒ¨",
            210: "æ ƒæœ¨åŒ—éƒ¨",
            215: "æ ƒæœ¨å—éƒ¨",
            220: "ç¾¤é¦¬åŒ—éƒ¨",
            225: "ç¾¤é¦¬å—éƒ¨",
            230: "åŸ¼ç‰åŒ—éƒ¨",
            231: "åŸ¼ç‰å—éƒ¨",
            232: "åŸ¼ç‰ç§©çˆ¶",
            240: "åƒè‘‰åŒ—æ±éƒ¨",
            241: "åƒè‘‰åŒ—è¥¿éƒ¨",
            242: "åƒè‘‰å—éƒ¨",
            250: "æ±äº¬",
            255: "ä¼Šè±†è«¸å³¶åŒ—éƒ¨",
            260: "ä¼Šè±†è«¸å³¶å—éƒ¨",
            265: "å°ç¬ åŸ",
            270: "ç¥å¥ˆå·æ±éƒ¨",
            275: "ç¥å¥ˆå·è¥¿éƒ¨",
            300: "æ–°æ½Ÿä¸Šè¶Š",
            301: "æ–°æ½Ÿä¸­è¶Š",
            302: "æ–°æ½Ÿä¸‹è¶Š",
            305: "æ–°æ½Ÿä½æ¸¡",
            310: "å¯Œå±±æ±éƒ¨",
            315: "å¯Œå±±è¥¿éƒ¨",
            320: "çŸ³å·èƒ½ç™»",
            325: "çŸ³å·åŠ è³€",
            330: "ç¦äº•å¶ºåŒ—",
            335: "ç¦äº•å¶ºå—",
            340: "å±±æ¢¨æ±éƒ¨",
            345: "å±±æ¢¨ä¸­ãƒ»è¥¿éƒ¨",
            350: "é•·é‡åŒ—éƒ¨",
            351: "é•·é‡ä¸­éƒ¨",
            355: "é•·é‡å—éƒ¨",
            400: "å²é˜œé£›é¨¨",
            405: "å²é˜œç¾æ¿ƒ",
            410: "é™å²¡ä¼Šè±†",
            411: "é™å²¡æ±éƒ¨",
            415: "é™å²¡ä¸­éƒ¨",
            416: "é™å²¡è¥¿éƒ¨",
            420: "æ„›çŸ¥æ±éƒ¨",
            425: "æ„›çŸ¥è¥¿éƒ¨",
            430: "ä¸‰é‡åŒ—ä¸­éƒ¨",
            435: "ä¸‰é‡å—éƒ¨",
            440: "æ»‹è³€åŒ—éƒ¨",
            445: "æ»‹è³€å—éƒ¨",
            450: "äº¬éƒ½åŒ—éƒ¨",
            455: "äº¬éƒ½å—éƒ¨",
            460: "å¤§é˜ªåŒ—éƒ¨",
            465: "å¤§é˜ªå—éƒ¨",
            470: "å…µåº«åŒ—éƒ¨",
            475: "å…µåº«å—éƒ¨",
            480: "å¥ˆè‰¯",
            490: "å’Œæ­Œå±±åŒ—éƒ¨",
            495: "å’Œæ­Œå±±å—éƒ¨",
            500: "é³¥å–æ±éƒ¨",
            505: "é³¥å–ä¸­ãƒ»è¥¿éƒ¨",
            510: "å³¶æ ¹æ±éƒ¨",
            515: "å³¶æ ¹è¥¿éƒ¨",
            514: "å³¶æ ¹éš å²",
            520: "å²¡å±±åŒ—éƒ¨",
            525: "å²¡å±±å—éƒ¨",
            530: "åºƒå³¶åŒ—éƒ¨",
            535: "åºƒå³¶å—éƒ¨",
            540: "å±±å£åŒ—éƒ¨",
            545: "å±±å£ä¸­ãƒ»æ±éƒ¨",
            541: "å±±å£è¥¿éƒ¨",
            550: "å¾³å³¶åŒ—éƒ¨",
            555: "å¾³å³¶å—éƒ¨",
            560: "é¦™å·",
            570: "æ„›åª›æ±äºˆ",
            575: "æ„›åª›ä¸­äºˆ",
            576: "æ„›åª›å—äºˆ",
            580: "é«˜çŸ¥æ±éƒ¨",
            581: "é«˜çŸ¥ä¸­éƒ¨",
            582: "é«˜çŸ¥è¥¿éƒ¨",
            600: "ç¦å²¡ç¦å²¡",
            601: "ç¦å²¡åŒ—ä¹å·",
            602: "ç¦å²¡ç­‘è±Š",
            605: "ç¦å²¡ç­‘å¾Œ",
            610: "ä½è³€åŒ—éƒ¨",
            615: "ä½è³€å—éƒ¨",
            620: "é•·å´åŒ—éƒ¨",
            625: "é•·å´å—éƒ¨",
            630: "é•·å´å£±å²ãƒ»å¯¾é¦¬",
            635: "é•·å´äº”å³¶",
            640: "ç†Šæœ¬é˜¿è˜‡",
            641: "ç†Šæœ¬ç†Šæœ¬",
            645: "ç†Šæœ¬çƒç£¨",
            646: "ç†Šæœ¬å¤©è‰ãƒ»èŠ¦åŒ—",
            650: "å¤§åˆ†åŒ—éƒ¨",
            651: "å¤§åˆ†ä¸­éƒ¨",
            655: "å¤§åˆ†è¥¿éƒ¨",
            656: "å¤§åˆ†å—éƒ¨",
            660: "å®®å´åŒ—éƒ¨å¹³é‡éƒ¨",
            661: "å®®å´åŒ—éƒ¨å±±æ²¿ã„",
            665: "å®®å´å—éƒ¨å¹³é‡éƒ¨",
            666: "å®®å´å—éƒ¨å±±æ²¿ã„",
            670: "é¹¿å…å³¶è–©æ‘©",
            675: "é¹¿å…å³¶å¤§éš…",
            680: "ç¨®å­å³¶ãƒ»å±‹ä¹…å³¶",
            685: "é¹¿å…å³¶å¥„ç¾",
            700: "æ²–ç¸„æœ¬å³¶åŒ—éƒ¨",
            701: "æ²–ç¸„æœ¬å³¶ä¸­å—éƒ¨",
            702: "æ²–ç¸„ä¹…ç±³å³¶",
            705: "æ²–ç¸„å…«é‡å±±",
            706: "æ²–ç¸„å®®å¤å³¶",
            710: "æ²–ç¸„å¤§æ±å³¶",
        }

    def _extract_content_without_timestamp(self, log_content: str) -> str:
        """æå–æ—¥å¿—å†…å®¹ä¸­æ’é™¤æ—¶é—´æˆ³çš„éƒ¨åˆ†ï¼Œç”¨äºé‡å¤æ£€æµ‹"""
        lines = log_content.split("\n")
        content_without_timestamp = []

        for line in lines:
            # æ’é™¤æ—¶é—´æˆ³è¡Œ
            if line.strip().startswith("ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´:"):
                continue
            content_without_timestamp.append(line)

        return "\n".join(content_without_timestamp)

    def _is_exact_duplicate_in_log(self, new_log_content: str) -> bool:
        """æ£€æŸ¥æœ€è¿‘çš„æ—¥å¿—ä¸­æ˜¯å¦å­˜åœ¨å®Œå…¨é‡å¤çš„å†…å®¹ï¼ˆåŸºäºå†…å­˜ç¼“å­˜ï¼‰"""
        try:
            # æå–æ–°å†…å®¹ä¸­æ’é™¤æ—¶é—´æˆ³çš„éƒ¨åˆ†
            new_content_clean = self._extract_content_without_timestamp(new_log_content)

            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if new_content_clean in self.recent_raw_logs:
                logger.debug("[ç¾å®³é¢„è­¦] å‘ç°å†…å®¹å®Œå…¨é‡å¤çš„æ—¥å¿—ï¼ˆå†…å­˜ç¼“å­˜ï¼‰ï¼Œè·³è¿‡å†™å…¥")
                return True

            # æ›´æ–°ç¼“å­˜
            self.recent_raw_logs.append(new_content_clean)
            if len(self.recent_raw_logs) > self.max_raw_log_cache:
                self.recent_raw_logs.pop(0)

            return False

        except Exception as e:
            logger.warning(f"[ç¾å®³é¢„è­¦] æ£€æŸ¥é‡å¤å†…å®¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå…è®¸å†™å…¥ï¼ˆä¸é˜»æ­¢ï¼‰
            return False

    def log_raw_message(
        self,
        source: str,
        message_type: str,
        raw_data: Any,
        connection_info: dict | None = None,
    ):
        """è®°å½•åŸå§‹æ¶ˆæ¯"""
        if not self.enabled:
            # ä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¾“å‡ºï¼Œé¿å…åˆ·å±
            # logger.debug(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨æœªå¯ç”¨ï¼Œè·³è¿‡è®°å½•: {source}")
            return

        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥æ¶ˆæ¯
            filter_reason = self._should_filter_message(raw_data, source)
            if filter_reason:
                # æ ¹æ®è¿‡æ»¤åŸå› å†³å®šæ—¥å¿—çº§åˆ«
                # å¿ƒè·³åŒ…ã€ç±»å‹è¿‡æ»¤ã€P2PèŠ‚ç‚¹çŠ¶æ€ã€é‡å¤äº‹ä»¶åˆ—è¡¨ç­‰é«˜é¢‘æ¶ˆæ¯ä½¿ç”¨DEBUGçº§åˆ«
                # è¿æ¥çŠ¶æ€ç­‰ä½¿ç”¨INFOçº§åˆ«
                is_high_frequency = any(
                    keyword in filter_reason
                    for keyword in ["æ¶ˆæ¯ç±»å‹è¿‡æ»¤", "P2PèŠ‚ç‚¹çŠ¶æ€", "å¿ƒè·³", "é‡å¤äº‹ä»¶"]
                )

                if is_high_frequency:
                    logger.debug(
                        f"[ç¾å®³é¢„è­¦] è¿‡æ»¤æ¶ˆæ¯ - æ¥æº: {source}, ç±»å‹: {message_type}, åŸå› : {filter_reason}"
                    )
                else:
                    logger.info(
                        f"[ç¾å®³é¢„è­¦] è¿‡æ»¤æ—¥å¿—æ¶ˆæ¯ - æ¥æº: {source}, ç±»å‹: {message_type}, åŸå› : {filter_reason}"
                    )

                self.filter_stats["total_filtered"] += 1
                return

            # è·å–å½“å‰æ—¶é—´
            current_time = datetime.now()

            # å‡†å¤‡æ—¥å¿—æ¡ç›®æ•°æ®
            log_entry = {
                "timestamp": current_time.isoformat(),
                "source": source,
                "message_type": message_type,
                "raw_data": raw_data,
                "connection_info": connection_info or {},
                "plugin_version": self.plugin_version,
            }

            # å°è¯•å¯è¯»æ€§æ ¼å¼åŒ–
            try:
                log_content = self._format_readable_log(log_entry)
            except Exception as format_error:
                # å¦‚æœæ–°æ ¼å¼å¤±è´¥ï¼Œå›é€€åˆ°å®‰å…¨çš„JSONæ ¼å¼
                logger.warning(
                    f"[ç¾å®³é¢„è­¦] å¯è¯»æ ¼å¼å¤±è´¥ï¼Œå›é€€åˆ°JSONæ ¼å¼: {format_error}"
                )
                log_content = (
                    json.dumps(log_entry, ensure_ascii=False, indent=2) + "\n\n"
                )

            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨100%å®Œå…¨é‡å¤çš„å†…å®¹ï¼ˆæ’é™¤æ—¶é—´æˆ³åï¼‰
            if self._is_exact_duplicate_in_log(log_content):
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] è·³è¿‡å†™å…¥å†…å®¹å®Œå…¨é‡å¤çš„æ—¥å¿— - æ¥æº: {source}, ç±»å‹: {message_type}"
                )
                return

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.log_file_path.parent.mkdir(parents=True, exist_ok=True)

            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(self.log_file_path, "a", encoding="utf-8") as f:
                f.write(log_content)
                f.flush()  # ç¡®ä¿ç«‹å³å†™å…¥ç£ç›˜

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¿…è¦æ—¶è¿›è¡Œè½®è½¬
            self._check_log_rotation()

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] è®°å½•åŸå§‹æ¶ˆæ¯å¤±è´¥: {e}")
            logger.error(
                f"[ç¾å®³é¢„è­¦] å¤±è´¥çš„æ¶ˆæ¯ - æ¥æº: {source}, ç±»å‹: {message_type}"
            )
            # è®°å½•å¼‚å¸¸å †æ ˆ
            logger.error(f"[ç¾å®³é¢„è­¦] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

    def log_websocket_message(
        self, connection_name: str, message: str, url: str | None = None
    ):
        """è®°å½•WebSocketæ¶ˆæ¯"""
        self.log_raw_message(
            source=f"websocket_{connection_name}",
            message_type="websocket_message",
            raw_data=message,
            connection_info={"url": url, "connection_type": "websocket"}
            if url
            else {"connection_type": "websocket"},
        )

    def log_tcp_message(self, server: str, port: int, message: str):
        """è®°å½•TCPæ¶ˆæ¯"""
        logger.info(
            f"[ç¾å®³é¢„è­¦] å‡†å¤‡è®°å½•TCPæ¶ˆæ¯ - æœåŠ¡å™¨: {server}:{port}, æ¶ˆæ¯: {message[:128]}..."
        )

        # å…ˆæ£€æŸ¥è¿‡æ»¤æƒ…å†µ
        filter_reason = self._should_filter_message(message)
        if filter_reason:
            logger.info(f"[ç¾å®³é¢„è­¦] TCPæ¶ˆæ¯è¢«è¿‡æ»¤ - åŸå› : {filter_reason}")
        else:
            logger.debug("[ç¾å®³é¢„è­¦] TCPæ¶ˆæ¯æœªè¢«è¿‡æ»¤ï¼Œå°†è®°å½•åˆ°æ—¥å¿—")

        self.log_raw_message(
            source="tcp_global_quake",
            message_type="tcp_message",
            raw_data=message,
            connection_info={"server": server, "port": port, "connection_type": "tcp"},
        )

    def log_http_response(
        self, url: str, response_data: Any, status_code: int | None = None
    ):
        """è®°å½•HTTPå“åº”"""
        self.log_raw_message(
            source="http_response",
            message_type="http_response",
            raw_data=response_data,
            connection_info={
                "url": url,
                "status_code": status_code,
                "connection_type": "http",
            },
        )

    def log_http_earthquake_list(
        self,
        source: str,
        url: str,
        earthquake_list: dict[str, Any],
        max_items: int = 5,
    ):
        """
        è®°å½• HTTP åœ°éœ‡åˆ—è¡¨å“åº”çš„æ‘˜è¦ï¼ˆä¸è®°å½•å®Œæ•´åˆ—è¡¨ï¼Œé¿å…æ—¥å¿—è†¨èƒ€ï¼‰

        Args:
            source: æ•°æ®æºæ ‡è¯†ï¼Œå¦‚ "http_wolfx_cenc" æˆ– "http_wolfx_jma"
            url: è¯·æ±‚çš„ URL
            earthquake_list: å®Œæ•´çš„åœ°éœ‡åˆ—è¡¨å“åº”æ•°æ®
            max_items: åªè®°å½•å‰å¤šå°‘æ¡äº‹ä»¶ï¼Œé»˜è®¤ 5 æ¡
        """
        if not self.enabled:
            return

        try:
            # æ„å»ºæ‘˜è¦æ•°æ®
            summary_data = {
                "summary": True,
                "message": f"åœ°éœ‡åˆ—è¡¨æ‘˜è¦ (ä»…æ˜¾ç¤ºå‰ {max_items} æ¡)",
            }

            # æå–äº‹ä»¶æ•°é‡ç»Ÿè®¡
            total_count = 0
            sample_events = []

            # Wolfx åˆ—è¡¨æ ¼å¼: {"No1": {...}, "No2": {...}, ...}
            # æŒ‰ç…§ No é”®çš„æ•°å­—æ’åº
            if isinstance(earthquake_list, dict):
                # è¿‡æ»¤å‡º No å¼€å¤´çš„é”®
                no_keys = [
                    k for k in earthquake_list.keys() if k.startswith("No")
                ]
                total_count = len(no_keys)

                # æŒ‰æ•°å­—æ’åºï¼ˆNo1, No2, ...ï¼‰
                sorted_keys = sorted(
                    no_keys, key=lambda x: int(x[2:]) if x[2:].isdigit() else 999
                )

                # åªå–å‰ max_items æ¡
                for key in sorted_keys[:max_items]:
                    event = earthquake_list.get(key, {})
                    if isinstance(event, dict):
                        # åªæå–å…³é”®å­—æ®µç”¨äºæ‘˜è¦
                        compact_event = {
                            "key": key,
                            "magnitude": event.get("Magnitude")
                            or event.get("magnitude"),
                            "place": event.get("Hypocenter")
                            or event.get("placeName")
                            or event.get("location"),
                            "time": event.get("OriginTime")
                            or event.get("shockTime")
                            or event.get("time"),
                            "depth": event.get("Depth") or event.get("depth"),
                        }
                        sample_events.append(compact_event)

            summary_data["total_events"] = total_count
            summary_data["sample_events"] = sample_events

            if total_count > max_items:
                summary_data[
                    "note"
                ] = f"è¿˜æœ‰ {total_count - max_items} æ¡äº‹ä»¶æœªæ˜¾ç¤º"

            # è®°å½•æ‘˜è¦
            self.log_raw_message(
                source=source,
                message_type="http_earthquake_list_summary",
                raw_data=summary_data,
                connection_info={
                    "url": url,
                    "method": "GET",
                    "connection_type": "http",
                    "summary_mode": True,
                },
            )

        except Exception as e:
            logger.warning(f"[ç¾å®³é¢„è­¦] åœ°éœ‡åˆ—è¡¨æ‘˜è¦è®°å½•å¤±è´¥: {e}")
            # å¤±è´¥æ—¶å›é€€åˆ°ç®€å•çš„ç»Ÿè®¡è®°å½•
            try:
                fallback_data = {
                    "error": "æ‘˜è¦ç”Ÿæˆå¤±è´¥",
                    "total_keys": len(earthquake_list)
                    if isinstance(earthquake_list, dict)
                    else 0,
                }
                self.log_raw_message(
                    source=source,
                    message_type="http_earthquake_list_summary",
                    raw_data=fallback_data,
                    connection_info={"url": url, "connection_type": "http"},
                )
            except Exception:
                pass

    def _check_log_rotation(self):
        """æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°å¹¶è¿›è¡Œè½®è½¬"""
        try:
            if not self.log_file_path.exists():
                return

            # è·å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
            file_size_mb = self.log_file_path.stat().st_size / (1024 * 1024)

            if file_size_mb > self.max_size_mb:
                self._rotate_logs()

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ—¥å¿—è½®è½¬æ£€æŸ¥å¤±è´¥: {e}")

    def _rotate_logs(self):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        try:
            # å…³é—­å½“å‰æ—¥å¿—æ–‡ä»¶
            for i in range(self.max_files - 1, 0, -1):
                old_file = self.log_file_path.with_suffix(f".log.{i}")
                new_file = self.log_file_path.with_suffix(f".log.{i + 1}")

                if old_file.exists():
                    if new_file.exists():
                        new_file.unlink()  # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
                    old_file.rename(new_file)

            # é‡å‘½åå½“å‰æ—¥å¿—æ–‡ä»¶
            if self.log_file_path.exists():
                backup_file = self.log_file_path.with_suffix(".log.1")
                if backup_file.exists():
                    backup_file.unlink()
                self.log_file_path.rename(backup_file)

            logger.info(f"[ç¾å®³é¢„è­¦] æ—¥å¿—æ–‡ä»¶å·²è½®è½¬ï¼Œå¤‡ä»½æ–‡ä»¶: {backup_file}")

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ—¥å¿—è½®è½¬å¤±è´¥: {e}")

    def get_log_summary(self) -> dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¯æŒæ–°å¯è¯»æ€§æ ¼å¼ï¼‰"""
        try:
            if not self.log_file_path.exists():
                return {"enabled": self.enabled, "log_exists": False}

            # ç»Ÿè®¡æ—¥å¿—æ¡ç›®
            entry_count = 0
            sources = set()
            date_range = {"start": None, "end": None}
            file_size_mb = self.log_file_path.stat().st_size / (1024 * 1024)

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(self.log_file_path, encoding="utf-8") as f:
                content = f.read()

            # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æ¡ç›®
            entries = content.split(f"\n{'=' * 35}\n")

            for entry in entries:
                entry = entry.strip()
                if not entry or not entry.startswith("ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´:"):
                    continue

                entry_count += 1

                try:
                    # æå–åŸºæœ¬ä¿¡æ¯
                    lines = entry.split("\n")
                    for line in lines:
                        line = line.strip()
                        if line.startswith("ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´:"):
                            timestamp_str = line.replace("ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´:", "").strip()
                            try:
                                dt = datetime.strptime(
                                    timestamp_str, "%Y-%m-%d %H:%M:%S"
                                )
                                if date_range[
                                    "start"
                                ] is None or dt < datetime.strptime(
                                    date_range["start"], "%Y-%m-%d %H:%M:%S"
                                ):
                                    date_range["start"] = timestamp_str
                                if date_range["end"] is None or dt > datetime.strptime(
                                    date_range["end"], "%Y-%m-%d %H:%M:%S"
                                ):
                                    date_range["end"] = timestamp_str
                            except ValueError:
                                pass
                        elif line.startswith("ğŸ“¡ æ¥æº:"):
                            source = line.replace("ğŸ“¡ æ¥æº:", "").strip()
                            sources.add(source)
                except Exception as e:
                    logger.debug(f"[ç¾å®³é¢„è­¦] è§£ææ—¥å¿—æ¡ç›®å¤±è´¥: {e}")
                    continue

            return {
                "enabled": self.enabled,
                "log_exists": True,
                "log_file": str(self.log_file_path),
                "total_entries": entry_count,
                "data_sources": list(sources),
                "date_range": date_range,
                "file_size_mb": file_size_mb,
                "filter_stats": self.filter_stats.copy(),
                "format_version": "3.0",  # æ–°æ ¼å¼ç‰ˆæœ¬
            }

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {"enabled": self.enabled, "log_exists": False, "error": str(e)}

    def clear_logs(self):
        """æ¸…é™¤æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        try:
            # åˆ é™¤ä¸»æ—¥å¿—æ–‡ä»¶
            if self.log_file_path.exists():
                self.log_file_path.unlink()

            # åˆ é™¤è½®è½¬çš„æ—§æ—¥å¿—æ–‡ä»¶
            for i in range(1, self.max_files + 1):
                old_file = self.log_file_path.with_suffix(f".log.{i}")
                if old_file.exists():
                    old_file.unlink()

            # æ¸…ç©ºå»é‡ç¼“å­˜
            self.recent_event_hashes.clear()

            # é‡ç½®ç»Ÿè®¡
            for key in self.filter_stats:
                self.filter_stats[key] = 0

            logger.info("[ç¾å®³é¢„è­¦] æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å·²æ¸…é™¤ï¼Œå»é‡ç¼“å­˜å·²æ¸…ç©º")

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ¸…é™¤æ—¥å¿—å¤±è´¥: {e}")

    def _get_plugin_version(self) -> str:
        """è·å–æ’ä»¶ç‰ˆæœ¬å·"""
        try:
            # å°è¯•ä» metadata.yaml è¯»å–
            metadata_path = Path(__file__).parent.parent / "metadata.yaml"
            if metadata_path.exists():
                with open(metadata_path, encoding="utf-8") as f:
                    # ç®€å•è§£æ YAMLï¼Œé¿å…å¼•å…¥ yaml ä¾èµ–
                    for line in f:
                        if line.strip().startswith("version:"):
                            return line.split(":", 1)[1].strip()
        except Exception:
            pass
        return "unknown"


# å‘åå…¼å®¹çš„å‡½æ•°
def get_message_logger(config: dict[str, Any], plugin_name: str) -> MessageLogger:
    """è·å–æ¶ˆæ¯è®°å½•å™¨å®ä¾‹"""
    return MessageLogger(config, plugin_name)
