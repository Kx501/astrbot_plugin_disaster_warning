"""
åŸå§‹æ¶ˆæ¯æ ¼å¼è®°å½•å™¨
ç”¨äºè®°å½•æ‰€æœ‰æ•°æ®æºçš„åŸå§‹æ¶ˆæ¯æ ¼å¼ï¼Œä¾¿äºåˆ†æå’Œå¼€å‘
"""

import json
from datetime import datetime
from typing import Any

from astrbot.api import logger
from astrbot.api.star import StarTools


class MessageLogger:
    """åŸå§‹æ¶ˆæ¯æ ¼å¼è®°å½•å™¨"""

    def __init__(self, config: dict[str, Any], plugin_name: str):
        self.config = config
        self.plugin_name = plugin_name
        self.enabled = config.get("debug_config", {}).get(
            "enable_raw_message_logging", False
        )
        self.log_file_name = config.get("debug_config", {}).get(
            "raw_message_log_path", "raw_messages.log"
        )
        self.max_size_mb = config.get("debug_config", {}).get("log_max_size_mb", 50)
        self.max_files = config.get("debug_config", {}).get("log_max_files", 5)

        # è¿‡æ»¤é…ç½®
        self.filter_heartbeat = config.get("debug_config", {}).get(
            "filter_heartbeat_messages", True
        )
        self.filter_types = config.get("debug_config", {}).get(
            "filtered_message_types",
            [
                "heartbeat",
                "ping",
                "pong",  # ç§»é™¤ "initial" å’Œ "update"ï¼Œå› ä¸ºå®é™…æ•°æ®æ¶ˆæ¯ä½¿ç”¨è¿™äº›ç±»å‹
            ],
        )

        # æ–°å¢è¿‡æ»¤è§„åˆ™
        self.filter_p2p_areas = config.get("debug_config", {}).get(
            "filter_p2p_areas_messages", True
        )
        self.filter_duplicate_events = config.get("debug_config", {}).get(
            "filter_duplicate_events", True
        )
        self.filter_connection_status = config.get("debug_config", {}).get(
            "filter_connection_status", True
        )

        # ç”¨äºå»é‡çš„ç¼“å­˜
        self.recent_event_hashes = set()
        self.max_cache_size = 1000

        # æ—¥å¿—è¿‡æ»¤ç»Ÿè®¡
        self.filter_stats = {
            "heartbeat_filtered": 0,
            "p2p_areas_filtered": 0,
            "duplicate_events_filtered": 0,
            "connection_status_filtered": 0,
            "total_filtered": 0,
        }

        # è·å–æ’ä»¶æ•°æ®ç›®å½•
        self.data_dir = StarTools.get_data_dir("astrbot_plugin_disaster_warning")
        self.log_file_path = self.data_dir / self.log_file_name

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {self.log_file_path}")
        if self.filter_heartbeat:
            logger.info("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è¿‡æ»¤é…ç½®:")
            logger.info(f"  - åŸºç¡€ç±»å‹è¿‡æ»¤: {self.filter_types}")
            logger.info(f"  - P2PèŠ‚ç‚¹çŠ¶æ€è¿‡æ»¤: {self.filter_p2p_areas}")
            logger.info(f"  - é‡å¤äº‹ä»¶è¿‡æ»¤: {self.filter_duplicate_events}")
            logger.info(f"  - è¿æ¥çŠ¶æ€è¿‡æ»¤: {self.filter_connection_status}")

    def _should_filter_message(self, raw_data: Any) -> str:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥æ¶ˆæ¯ï¼Œè¿”å›è¿‡æ»¤åŸå› ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸è¿‡æ»¤"""
        if not self.filter_heartbeat:
            return ""

        try:
            # å¤„ç†ä¸åŒç±»å‹çš„åŸå§‹æ•°æ®
            if isinstance(raw_data, str) and raw_data.strip():
                # å°è¯•è§£æJSONæ•°æ®
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯ä½†ä¸è¿‡æ»¤
                    logger.debug(
                        f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - JSONè§£æå¤±è´¥ï¼Œæ¶ˆæ¯å‰100å­—ç¬¦: {raw_data[:100]}..."
                    )
                    return ""

                # è·å–æ¶ˆæ¯ç±»å‹ç”¨äºè°ƒè¯•
                msg_type = data.get("type", "")
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ£€æŸ¥æ¶ˆæ¯è¿‡æ»¤ï¼Œç±»å‹: {msg_type}, æ•°æ®é•¿åº¦: {len(raw_data)}"
                )

                # æ£€æŸ¥æ¶ˆæ¯ç±»å‹
                if msg_type and msg_type.lower() in self.filter_types:
                    self.filter_stats["heartbeat_filtered"] += 1
                    logger.debug(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}")
                    return f"æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}"

                # æ£€æŸ¥P2P areasæ¶ˆæ¯ï¼ˆèŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯ï¼‰
                if self.filter_p2p_areas and self._is_p2p_areas_message(data):
                    self.filter_stats["p2p_areas_filtered"] += 1
                    logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯è¿‡æ»¤")
                    return "P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥é‡å¤äº‹ä»¶ - æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                if self.filter_duplicate_events:
                    event_hash = self._generate_event_hash(data)
                    is_duplicate = self._is_duplicate_event(data)
                    if is_duplicate:
                        self.filter_stats["duplicate_events_filtered"] += 1
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - é‡å¤äº‹ä»¶è¿‡æ»¤ï¼Œå“ˆå¸Œ: {event_hash}, åŸå› : äº‹ä»¶å“ˆå¸Œå·²å­˜åœ¨"
                        )
                        return f"é‡å¤äº‹ä»¶ (å“ˆå¸Œ: {event_hash})"
                    elif event_hash:
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - äº‹ä»¶å“ˆå¸Œç”Ÿæˆ: {event_hash}, å…è®¸è®°å½•"
                        )

                # æ£€æŸ¥è¿æ¥çŠ¶æ€æ¶ˆæ¯
                if self.filter_connection_status and self._is_connection_status_message(
                    data
                ):
                    self.filter_stats["connection_status_filtered"] += 1
                    logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - è¿æ¥çŠ¶æ€æ¶ˆæ¯è¿‡æ»¤")
                    return "è¿æ¥çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥WebSocketæ¶ˆæ¯å†…å®¹ï¼ˆåµŒå¥—JSONï¼‰
                if "raw_data" in data and isinstance(data["raw_data"], str):
                    try:
                        inner_data = json.loads(data["raw_data"])
                        inner_type = inner_data.get("type", "").lower()
                        if inner_type in self.filter_types:
                            self.filter_stats["heartbeat_filtered"] += 1
                            return f"å†…å±‚æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {inner_type}"

                        # æ£€æŸ¥å†…å±‚æ•°æ®çš„P2P areasæ¶ˆæ¯
                        if self.filter_p2p_areas and self._is_p2p_areas_message(
                            inner_data
                        ):
                            self.filter_stats["p2p_areas_filtered"] += 1
                            return "å†…å±‚P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                        # æ£€æŸ¥å†…å±‚æ•°æ®çš„é‡å¤äº‹ä»¶
                        if self.filter_duplicate_events and self._is_duplicate_event(
                            inner_data
                        ):
                            self.filter_stats["duplicate_events_filtered"] += 1
                            return "å†…å±‚é‡å¤äº‹ä»¶"
                    except (json.JSONDecodeError, AttributeError):
                        pass

            elif isinstance(raw_data, dict):
                # å¦‚æœraw_dataå·²ç»æ˜¯å­—å…¸
                msg_type = raw_data.get("type", "")
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ£€æŸ¥å­—å…¸ç±»å‹æ¶ˆæ¯ï¼Œç±»å‹: {msg_type}"
                )

                if msg_type and msg_type.lower() in self.filter_types:
                    self.filter_stats["heartbeat_filtered"] += 1
                    logger.debug(f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}")
                    return f"æ¶ˆæ¯ç±»å‹è¿‡æ»¤: {msg_type}"

                # æ£€æŸ¥P2P areasæ¶ˆæ¯
                if self.filter_p2p_areas and self._is_p2p_areas_message(raw_data):
                    self.filter_stats["p2p_areas_filtered"] += 1
                    logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯è¿‡æ»¤")
                    return "P2PèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯"

                # æ£€æŸ¥é‡å¤äº‹ä»¶ - æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                if self.filter_duplicate_events:
                    event_hash = self._generate_event_hash(raw_data)
                    is_duplicate = self._is_duplicate_event(raw_data)
                    if is_duplicate:
                        self.filter_stats["duplicate_events_filtered"] += 1
                        logger.debug(
                            f"[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - é‡å¤äº‹ä»¶è¿‡æ»¤ï¼Œå“ˆå¸Œ: {event_hash}"
                        )
                        return f"é‡å¤äº‹ä»¶ (å“ˆå¸Œ: {event_hash})"

                # æ£€æŸ¥è¿æ¥çŠ¶æ€æ¶ˆæ¯
                if self.filter_connection_status and self._is_connection_status_message(
                    raw_data
                ):
                    self.filter_stats["connection_status_filtered"] += 1
                    logger.debug("[ç¾å®³é¢„è­¦] æ¶ˆæ¯è®°å½•å™¨ - è¿æ¥çŠ¶æ€æ¶ˆæ¯è¿‡æ»¤")
                    return "è¿æ¥çŠ¶æ€æ¶ˆæ¯"

        except (json.JSONDecodeError, KeyError, TypeError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œä¸è¿‡æ»¤
            pass

        return ""

    def _is_p2p_areas_message(self, data: dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºP2P areasæ¶ˆæ¯ï¼ˆèŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯ï¼‰"""
        # P2Pæ¶ˆæ¯é€šå¸¸åŒ…å«areasæ•°ç»„ï¼Œè®°å½•å„ä¸ªIDçš„peeræ•°é‡
        if "areas" in data and isinstance(data["areas"], list):
            # æ£€æŸ¥areasæ•°ç»„çš„å†…å®¹ï¼Œå¦‚æœä¸»è¦æ˜¯peeræ•°é‡ä¿¡æ¯ï¼Œåˆ™è¿‡æ»¤
            areas = data["areas"]
            if areas and all(
                isinstance(area, dict) and "peer" in area for area in areas[:3]
            ):
                return True
        return False

    def _is_duplicate_event(self, data: dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤äº‹ä»¶"""
        try:
            # ç”Ÿæˆäº‹ä»¶å“ˆå¸Œï¼ˆåŸºäºå…³é”®å­—æ®µï¼‰
            event_hash = self._generate_event_hash(data)
            if event_hash in self.recent_event_hashes:
                return True

            # æ·»åŠ åˆ°ç¼“å­˜ï¼ˆLRUé£æ ¼ï¼‰
            if len(self.recent_event_hashes) >= self.max_cache_size:
                # ç§»é™¤æœ€æ—§çš„æ¡ç›®ï¼ˆç®€å•å®ç°ï¼‰
                self.recent_event_hashes.pop()
            self.recent_event_hashes.add(event_hash)

            return False

        except Exception:
            return False

    def _generate_event_hash(self, data: dict[str, Any]) -> str:
        """ç”Ÿæˆäº‹ä»¶å“ˆå¸Œç”¨äºå»é‡ - æ™ºèƒ½è¯†åˆ«äº‹ä»¶ç±»å‹ï¼Œé¿å…è¯¯åˆ¤"""
        # åŸºäºäº‹ä»¶çš„å…³é”®å­—æ®µç”Ÿæˆå“ˆå¸Œ
        hash_parts = []

        # é¦–å…ˆè¿›è¡Œäº‹ä»¶ç±»å‹æ™ºèƒ½è¯†åˆ«
        event_type = self._detect_event_type(data)
        hash_parts.append(f"etype:{event_type}")

        # ä¸åŒç±»å‹çš„äº‹ä»¶ä½¿ç”¨ä¸åŒçš„å»é‡ç­–ç•¥
        if event_type == "weather":
            # æ°”è±¡é¢„è­¦ï¼šä¸»è¦åŸºäºIDå’Œæ—¶é—´
            return self._generate_weather_hash(data, hash_parts)
        elif event_type == "earthquake":
            # åœ°éœ‡äº‹ä»¶ï¼šåŸºäºä½ç½®ã€éœ‡çº§ã€æ—¶é—´çš„ç»¼åˆåˆ¤æ–­
            return self._generate_earthquake_hash(data, hash_parts)
        elif event_type == "tsunami":
            # æµ·å•¸é¢„è­¦ï¼šåŸºäºåŒºåŸŸå’Œæ—¶é—´
            return self._generate_tsunami_hash(data, hash_parts)
        else:
            # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨é€šç”¨å“ˆå¸Œ
            return self._generate_generic_hash(data, hash_parts)

    def _detect_event_type(self, data: dict[str, Any]) -> str:
        """æ™ºèƒ½æ£€æµ‹äº‹ä»¶ç±»å‹"""
        # æ£€æŸ¥æ¶ˆæ¯ç±»å‹å­—æ®µ
        msg_type = str(data.get("type", "")).lower()

        # ä½¿ç”¨msg_typeè¿›è¡Œäº‹ä»¶ç±»å‹åˆ¤æ–­
        if msg_type in ["weather", "alarm", "warning"]:
            return "weather"
        elif msg_type in ["earthquake", "seismic"]:
            return "earthquake"
        elif msg_type in ["tsunami"]:
            return "tsunami"

        # æ£€æŸ¥å…¶ä»–å…³é”®å­—æ®µ
        data_str = str(data).lower()

        # æ°”è±¡é¢„è­¦ç‰¹å¾
        if any(
            keyword in data_str
            for keyword in ["weather", "alarm", "é¢„è­¦", "warning", "headline"]
        ):
            if (
                "åœ°éœ‡" not in data_str
                and "earthquake" not in data_str
                and "magnitude" not in data_str
            ):
                return "weather"

        # åœ°éœ‡äº‹ä»¶ç‰¹å¾
        if any(
            keyword in data_str
            for keyword in [
                "earthquake",
                "åœ°éœ‡",
                "magnitude",
                "éœ‡çº§",
                "hypocenter",
                "éœ‡æº",
            ]
        ):
            return "earthquake"

        # æµ·å•¸é¢„è­¦ç‰¹å¾
        if any(keyword in data_str for keyword in ["tsunami", "æµ·å•¸", "æ´¥æ³¢"]):
            return "tsunami"

        # P2Påœ°éœ‡ä¿¡æ¯
        if "code" in data and isinstance(data.get("code"), int):
            code = data["code"]
            if code in [551, 552, 556]:  # åœ°éœ‡æƒ…å ±ã€æ´¥æ³¢äºˆå ±ã€ç·Šæ€¥åœ°éœ‡é€Ÿå ±
                return "earthquake" if code in [551, 556] else "tsunami"

        # é»˜è®¤è¿”å›é€šç”¨ç±»å‹
        return "generic"

    def _generate_weather_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆæ°”è±¡é¢„è­¦å“ˆå¸Œ"""
        # æ°”è±¡é¢„è­¦ä¸»è¦åŸºäºIDå’Œå‘å¸ƒæ—¶é—´
        event_id = (
            data.get("id") or data.get("headline", "")[:50]
        )  # ä½¿ç”¨å‰50ä¸ªå­—ç¬¦ä½œä¸ºID
        if event_id:
            hash_parts.append(f"weather_id:{event_id}")

        # æ·»åŠ å‘å¸ƒæ—¶é—´ï¼ˆç²¾ç¡®åˆ°å°æ—¶ï¼‰
        time_info = data.get("effective") or data.get("issue_time") or data.get("time")
        if time_info:
            try:
                if isinstance(time_info, str) and len(time_info) >= 13:
                    # å–åˆ°å°æ—¶çº§åˆ«
                    time_key = time_info[:13]
                    hash_parts.append(f"weather_time:{time_key}")
            except Exception:
                pass

        return "|".join(hash_parts) if hash_parts else ""

    def _generate_earthquake_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆåœ°éœ‡äº‹ä»¶å“ˆå¸Œ - æ›´å®½æ¾çš„ç²¾åº¦"""
        # æ£€æŸ¥æ˜¯å¦æœ‰äº‹ä»¶ID
        event_id = data.get("id") or data.get("eventId") or data.get("EventID")
        if event_id:
            hash_parts.append(f"eq_id:{event_id}")
            # å¦‚æœæœ‰IDï¼Œå¯ä»¥è¿”å›ï¼Œå› ä¸ºIDé€šå¸¸æ˜¯å”¯ä¸€çš„
            return "|".join(hash_parts)

        # æ£€æŸ¥æ—¶é—´ä¿¡æ¯ - ä½¿ç”¨æ›´ç²—çš„ç²’åº¦ï¼ˆ10åˆ†é’Ÿçª—å£ï¼‰
        time_info = data.get("shockTime") or data.get("time") or data.get("OriginTime")
        if time_info:
            try:
                if isinstance(time_info, str):
                    # è§£ææ—¶é—´å¹¶é‡åŒ–åˆ°10åˆ†é’Ÿçº§åˆ«
                    time_obj = self._parse_datetime_for_hash(time_info)
                    if time_obj:
                        # é‡åŒ–åˆ°10åˆ†é’Ÿçº§åˆ«
                        minute_rounded = (time_obj.minute // 10) * 10
                        time_key = f"{time_obj.year}{time_obj.month:02d}{time_obj.day:02d}{time_obj.hour:02d}{minute_rounded:02d}"
                        hash_parts.append(f"eq_time:{time_key}")
            except Exception:
                pass

        # æ£€æŸ¥ä½ç½®ä¿¡æ¯ - ä½¿ç”¨æ›´å®½æ¾çš„ç²¾åº¦ï¼ˆ0.5åº¦ï¼Œçº¦55kmï¼‰
        lat = data.get("latitude") or data.get("Latitude")
        lon = data.get("longitude") or data.get("Longitude")
        if lat is not None and lon is not None:
            try:
                lat_val = float(lat)
                lon_val = float(lon)
                # 0.5åº¦ç²¾åº¦ï¼ˆçº¦55kmï¼‰
                lat_rounded = round(lat_val * 2) / 2
                lon_rounded = round(lon_val * 2) / 2
                hash_parts.append(f"eq_loc:{lat_rounded},{lon_rounded}")
            except (ValueError, TypeError):
                pass

        # æ£€æŸ¥éœ‡çº§ä¿¡æ¯ - ä½¿ç”¨æ•´æ•°çº§åˆ«
        magnitude = data.get("magnitude") or data.get("Magnitude")
        if magnitude is not None:
            try:
                mag_int = int(float(magnitude))
                hash_parts.append(f"eq_mag:{mag_int}")
            except (ValueError, TypeError):
                pass

        return "|".join(hash_parts) if hash_parts else ""

    def _generate_tsunami_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆæµ·å•¸é¢„è­¦å“ˆå¸Œ"""
        # åŸºäºé¢„è­¦åŒºåŸŸå’Œæ—¶é—´
        event_id = data.get("id") or data.get("code", "")
        if event_id:
            hash_parts.append(f"tsunami_id:{event_id}")

        # æ·»åŠ å‘å¸ƒæ—¶é—´ï¼ˆç²¾ç¡®åˆ°å°æ—¶ï¼‰
        time_info = data.get("issue_time") or data.get("time") or data.get("effective")
        if time_info:
            try:
                if isinstance(time_info, str) and len(time_info) >= 13:
                    time_key = time_info[:13]
                    hash_parts.append(f"tsunami_time:{time_key}")
            except Exception:
                pass

        return "|".join(hash_parts) if hash_parts else ""

    def _generate_generic_hash(self, data: dict[str, Any], hash_parts: list) -> str:
        """ç”Ÿæˆé€šç”¨å“ˆå¸Œ"""
        # å›é€€åˆ°åŸºç¡€å­—æ®µ
        event_id = data.get("id") or data.get("eventId") or data.get("EventID")
        if event_id:
            hash_parts.append(f"generic_id:{event_id}")

        return "|".join(hash_parts) if hash_parts else ""

    def _parse_datetime_for_hash(self, time_str: str) -> datetime | None:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ç”¨äºå“ˆå¸Œç”Ÿæˆ - æ›´å®½æ¾çš„è§£æ"""
        if not time_str:
            return None

        # å°è¯•å¤šç§æ ¼å¼
        formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%Y/%m/%d %H:%M",
            "%Y-%m-%d",
            "%Y/%m/%d",
        ]

        for fmt in formats:
            try:
                return datetime.strptime(time_str.strip(), fmt)
            except ValueError:
                continue

        return None

    def _is_connection_status_message(self, data: dict[str, Any]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¿æ¥çŠ¶æ€æ¶ˆæ¯"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¿æ¥å»ºç«‹ã€æ–­å¼€ç­‰çŠ¶æ€æ¶ˆæ¯
        msg_type = data.get("type", "").lower()
        if msg_type in ["connect", "disconnect", "connection", "status"]:
            return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿æ¥ç›¸å…³çš„å…³é”®è¯
        connection_keywords = [
            "connected",
            "disconnected",
            "connection",
            "status",
            "online",
            "offline",
        ]
        message_str = str(data).lower()
        if any(keyword in message_str for keyword in connection_keywords):
            # è¿›ä¸€æ­¥æ£€æŸ¥ï¼Œç¡®ä¿ä¸æ˜¯å®é™…çš„ç¾å®³äº‹ä»¶
            disaster_keywords = [
                "earthquake",
                "åœ°éœ‡",
                "éœ‡çº§",
                "magnitude",
                "tsunami",
                "æµ·å•¸",
                "weather",
                "æ°”è±¡",
            ]
            if not any(keyword in message_str for keyword in disaster_keywords):
                return True

        return False

    def _format_readable_log(self, log_entry: dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¯è¯»æ€§å¼ºçš„æ—¥å¿—å†…å®¹"""
        try:
            # åŸºç¡€ä¿¡æ¯æ ¼å¼åŒ–
            timestamp = datetime.fromisoformat(log_entry["timestamp"]).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            source = log_entry["source"]
            message_type = log_entry["message_type"]

            # æ„å»ºå¯è¯»æ€§å¼ºçš„æ—¥å¿—å¤´éƒ¨
            log_content = f"\n{'=' * 40}\n"
            log_content += f"ğŸ• æ—¥å¿—å†™å…¥æ—¶é—´: {timestamp}\n"
            log_content += f"ğŸ“¡ æ¥æº: {source}\n"
            log_content += f"ğŸ“‹ ç±»å‹: {message_type}\n"

            # æ·»åŠ è¿æ¥ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            connection_info = log_entry.get("connection_info", {})
            if connection_info:
                log_content += "ğŸ”— è¿æ¥: "
                if "url" in connection_info:
                    log_content += f"URL: {connection_info['url']}"
                elif "server" in connection_info and "port" in connection_info:
                    log_content += (
                        f"æœåŠ¡å™¨: {connection_info['server']}:{connection_info['port']}"
                    )
                log_content += "\n"

            # æ ¼å¼åŒ–åŸå§‹æ•°æ®
            raw_data = log_entry["raw_data"]
            log_content += "\nğŸ“Š åŸå§‹æ•°æ®:\n"

            # æ ¹æ®æ•°æ®ç±»å‹è¿›è¡Œä¸åŒçš„æ ¼å¼åŒ–
            if isinstance(raw_data, str):
                # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                try:
                    parsed_data = json.loads(raw_data)
                    log_content += self._format_json_data(parsed_data, indent=2)
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æ˜¾ç¤º
                    log_content += f"  {raw_data}\n"
            elif isinstance(raw_data, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                log_content += self._format_json_data(raw_data, indent=2)
            else:
                # å…¶ä»–æ ¼å¼
                log_content += f"  {str(raw_data)}\n"

            # æ·»åŠ æ’ä»¶ä¿¡æ¯
            log_content += (
                f"\nğŸ”§ æ’ä»¶ç‰ˆæœ¬: {log_entry.get('plugin_version', 'unknown')}\n"
            )
            log_content += f"{'=' * 40}\n"

            return log_content

        except Exception as e:
            # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•çš„JSONæ ¼å¼
            logger.warning(f"[ç¾å®³é¢„è­¦] æ—¥å¿—æ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ ¼å¼: {e}")
            return json.dumps(log_entry, ensure_ascii=False, indent=2) + "\n\n"

    def _format_json_data(self, data: dict[str, Any], indent: int = 0) -> str:
        """é€’å½’æ ¼å¼åŒ–JSONæ•°æ®ï¼Œå¢åŠ å¯è¯»æ€§"""
        result = ""
        indent_str = "  " * indent

        for key, value in data.items():
            # é”®åç¿»è¯‘å’Œæ ¼å¼åŒ–
            key_display = self._get_display_key(key)

            if isinstance(value, dict):
                result += f"{indent_str}ğŸ“‹ {key_display}:\n"
                result += self._format_json_data(value, indent + 1)
            elif isinstance(value, list):
                if len(value) > 0:
                    result += f"{indent_str}ğŸ“‹ {key_display} ({len(value)}é¡¹):\n"
                    for i, item in enumerate(value[:5]):  # åªæ˜¾ç¤ºå‰5é¡¹
                        if isinstance(item, dict):
                            result += f"{indent_str}  [{i + 1}]:\n"
                            result += self._format_json_data(item, indent + 2)
                        else:
                            result += f"{indent_str}  [{i + 1}]: {item}\n"
                    if len(value) > 5:
                        result += f"{indent_str}  ... è¿˜æœ‰ {len(value) - 5} é¡¹\n"
                else:
                    result += f"{indent_str}ğŸ“‹ {key_display}: []\n"
            else:
                # æ ¼å¼åŒ–å…·ä½“å€¼
                value_display = self._format_value(key, value)
                result += f"{indent_str}ğŸ“‹ {key_display}: {value_display}\n"

        return result

    def _get_display_key(self, key: str) -> str:
        """è·å–æ ¼å¼åŒ–çš„é”®åæ˜¾ç¤º"""
        key_mappings = {
            # P2Pç›¸å…³
            "code": "æ¶ˆæ¯ä»£ç ",
            "earthquake": "åœ°éœ‡ä¿¡æ¯",
            "hypocenter": "éœ‡æºä¿¡æ¯",
            "magnitude": "éœ‡çº§",
            "depth": "æ·±åº¦(km)",
            "latitude": "çº¬åº¦",
            "longitude": "ç»åº¦",
            "name": "åœ°ç‚¹åç§°",
            "time": "å‘ç”Ÿæ—¶é—´",
            "maxScale": "æœ€å¤§éœ‡åº¦(åŸå§‹)",
            "domesticTsunami": "æ—¥æœ¬å¢ƒå†…æµ·å•¸",
            "foreignTsunami": "æµ·å¤–æµ·å•¸",
            # JMAç›¸å…³
            "EventID": "äº‹ä»¶ID",
            "OriginTime": "å‘éœ‡æ—¶é—´",
            "Hypocenter": "éœ‡æºåœ°å",
            "MaxIntensity": "æœ€å¤§éœ‡åº¦",
            "Serial": "æŠ¥åºå·",
            "AnnouncedTime": "å‘å¸ƒæ—¶é—´",
            "isFinal": "æœ€ç»ˆæŠ¥",
            "isCancel": "å–æ¶ˆæŠ¥",
            # é€šç”¨
            "id": "ID",
            "_id": "æ•°æ®åº“ID",
            "type": "æ¶ˆæ¯ç±»å‹",
            "title": "æ ‡é¢˜",
            "source": "æ•°æ®æ¥æº",
            "status": "çŠ¶æ€",
            "issue": "å‘å¸ƒä¿¡æ¯",
            "correct": "è®¢æ­£ä¿¡æ¯",
            "placeName": "åœ°å",
            "shockTime": "å‘éœ‡æ—¶é—´",
            "createTime": "åˆ›å»ºæ—¶é—´",
            "infoTypeName": "ä¿¡æ¯ç±»å‹",
            "updates": "æ›´æ–°æ¬¡æ•°",
            "is_training": "è®­ç»ƒæ¨¡å¼",
            # è¿æ¥ä¿¡æ¯
            "url": "è¿æ¥åœ°å€",
            "connection_type": "è¿æ¥ç±»å‹",
            "server": "æœåŠ¡å™¨",
            "port": "ç«¯å£",
            "status_code": "çŠ¶æ€ç ",
        }

        return key_mappings.get(key, key)

    def _format_value(self, key: str, value: Any) -> str:
        """æ ¼å¼åŒ–å…·ä½“å€¼"""
        if value is None:
            return "æ— æ•°æ®"
        elif value == "":
            return "ç©ºå­—ç¬¦ä¸²"
        elif isinstance(value, (int, float)):
            # ç‰¹æ®Šæ•°å€¼æ ¼å¼åŒ–
            if key == "maxScale" and isinstance(value, int):
                scale_map = {
                    10: "éœ‡åº¦1",
                    20: "éœ‡åº¦2",
                    30: "éœ‡åº¦3",
                    40: "éœ‡åº¦4",
                    45: "éœ‡åº¦5å¼±",
                    50: "éœ‡åº¦5å¼·",
                    55: "éœ‡åº¦6å¼±",
                    60: "éœ‡åº¦6å¼·",
                    70: "éœ‡åº¦7",
                }
                return f"{value} ({scale_map.get(value, 'æœªçŸ¥')})"
            elif key in ["magnitude", "Magnitude"] and isinstance(value, (int, float)):
                return f"M{value}"
            elif key in ["depth", "Depth"] and isinstance(value, (int, float)):
                return f"{value}km"
            else:
                return str(value)
        elif isinstance(value, str):
            # å­—ç¬¦ä¸²é•¿åº¦æ§åˆ¶
            if len(value) > 50:
                return f"{value[:47]}..."
            return value
        else:
            return str(value)

    def log_raw_message(
        self,
        source: str,
        message_type: str,
        raw_data: Any,
        connection_info: dict | None = None,
    ):
        """è®°å½•åŸå§‹æ¶ˆæ¯ï¼ˆä¼˜åŒ–å¯è¯»æ€§æ ¼å¼ + å¼‚å¸¸å›é€€æœºåˆ¶ï¼‰"""
        if not self.enabled:
            return

        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿‡æ»¤è¯¥æ¶ˆæ¯
            filter_reason = self._should_filter_message(raw_data)
            if filter_reason:
                logger.debug(
                    f"[ç¾å®³é¢„è­¦] è¿‡æ»¤æ¶ˆæ¯ - æ¥æº: {source}, ç±»å‹: {message_type}, åŸå› : {filter_reason}"
                )
                self.filter_stats["total_filtered"] += 1
                return

            # è·å–å½“å‰æ—¶é—´
            current_time = datetime.now()

            # å‡†å¤‡æ—¥å¿—æ¡ç›®æ•°æ®
            log_entry = {
                "timestamp": current_time.isoformat(),
                "source": source,
                "message_type": message_type,
                "raw_data": raw_data,
                "connection_info": connection_info or {},
                "plugin_version": "1.0.0",
            }

            # å°è¯•æ–°çš„å¯è¯»æ€§æ ¼å¼åŒ–
            try:
                log_content = self._format_readable_log(log_entry)
            except Exception as format_error:
                # å¦‚æœæ–°æ ¼å¼å¤±è´¥ï¼Œå›é€€åˆ°å®‰å…¨çš„JSONæ ¼å¼
                logger.warning(
                    f"[ç¾å®³é¢„è­¦] å¯è¯»æ ¼å¼å¤±è´¥ï¼Œå›é€€åˆ°JSONæ ¼å¼: {format_error}"
                )
                log_content = (
                    json.dumps(log_entry, ensure_ascii=False, indent=2) + "\n\n"
                )

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.log_file_path.parent.mkdir(parents=True, exist_ok=True)

            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            with open(self.log_file_path, "a", encoding="utf-8") as f:
                f.write(log_content)

            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¿…è¦æ—¶è¿›è¡Œè½®è½¬
            self._check_log_rotation()

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] è®°å½•åŸå§‹æ¶ˆæ¯å¤±è´¥: {e}")
            logger.error(
                f"[ç¾å®³é¢„è­¦] å¤±è´¥çš„æ¶ˆæ¯ - æ¥æº: {source}, ç±»å‹: {message_type}"
            )
            # è®°å½•å¼‚å¸¸å †æ ˆ
            import traceback

            logger.error(f"[ç¾å®³é¢„è­¦] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

    def log_websocket_message(
        self, connection_name: str, message: str, url: str | None = None
    ):
        """è®°å½•WebSocketæ¶ˆæ¯"""
        self.log_raw_message(
            source=f"websocket_{connection_name}",
            message_type="websocket_message",
            raw_data=message,
            connection_info={"url": url, "connection_type": "websocket"},
        )

    def log_tcp_message(self, server: str, port: int, message: str):
        """è®°å½•TCPæ¶ˆæ¯"""
        logger.debug(
            f"[ç¾å®³é¢„è­¦] å‡†å¤‡è®°å½•TCPæ¶ˆæ¯ - æœåŠ¡å™¨: {server}:{port}, æ¶ˆæ¯: {message[:128]}..."
        )

        # å…ˆæ£€æŸ¥è¿‡æ»¤æƒ…å†µ
        filter_reason = self._should_filter_message(message)
        if filter_reason:
            logger.debug(f"[ç¾å®³é¢„è­¦] TCPæ¶ˆæ¯è¢«è¿‡æ»¤ - åŸå› : {filter_reason}")
        else:
            logger.debug("[ç¾å®³é¢„è­¦] TCPæ¶ˆæ¯æœªè¢«è¿‡æ»¤ï¼Œå°†è®°å½•åˆ°æ—¥å¿—")

        self.log_raw_message(
            source="tcp_global_quake",
            message_type="tcp_message",
            raw_data=message,
            connection_info={"server": server, "port": port, "connection_type": "tcp"},
        )

    def log_http_response(
        self, url: str, response_data: Any, status_code: int | None = None
    ):
        """è®°å½•HTTPå“åº”"""
        self.log_raw_message(
            source="http_response",
            message_type="http_response",
            raw_data=response_data,
            connection_info={
                "url": url,
                "status_code": status_code,
                "connection_type": "http",
            },
        )

    def _check_log_rotation(self):
        """æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°å¹¶è¿›è¡Œè½®è½¬"""
        try:
            if not self.log_file_path.exists():
                return

            # è·å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
            file_size_mb = self.log_file_path.stat().st_size / (1024 * 1024)

            if file_size_mb > self.max_size_mb:
                self._rotate_logs()

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ—¥å¿—è½®è½¬æ£€æŸ¥å¤±è´¥: {e}")

    def _rotate_logs(self):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        try:
            # å…³é—­å½“å‰æ—¥å¿—æ–‡ä»¶
            for i in range(self.max_files - 1, 0, -1):
                old_file = self.log_file_path.with_suffix(f".log.{i}")
                new_file = self.log_file_path.with_suffix(f".log.{i + 1}")

                if old_file.exists():
                    if new_file.exists():
                        new_file.unlink()  # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
                    old_file.rename(new_file)

            # é‡å‘½åå½“å‰æ—¥å¿—æ–‡ä»¶
            if self.log_file_path.exists():
                backup_file = self.log_file_path.with_suffix(".log.1")
                if backup_file.exists():
                    backup_file.unlink()
                self.log_file_path.rename(backup_file)

            logger.info(f"[ç¾å®³é¢„è­¦] æ—¥å¿—æ–‡ä»¶å·²è½®è½¬ï¼Œå¤‡ä»½æ–‡ä»¶: {backup_file}")

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ—¥å¿—è½®è½¬å¤±è´¥: {e}")

    def get_log_summary(self) -> dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ”¯æŒæ–°å¯è¯»æ€§æ ¼å¼ï¼‰"""
        try:
            if not self.log_file_path.exists():
                return {"enabled": self.enabled, "log_exists": False}

            # ç»Ÿè®¡æ—¥å¿—æ¡ç›®
            entry_count = 0
            sources = set()
            date_range = {"start": None, "end": None}
            file_size_mb = self.log_file_path.stat().st_size / (1024 * 1024)

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(self.log_file_path, encoding="utf-8") as f:
                content = f.read()

            # æŒ‰åˆ†éš”ç¬¦åˆ†å‰²æ¡ç›®
            entries = content.split(f"\n{'=' * 40}\n")

            for entry in entries:
                entry = entry.strip()
                if not entry or not entry.startswith("ğŸ• æ—¶é—´:"):
                    continue

                entry_count += 1

                try:
                    # æå–åŸºæœ¬ä¿¡æ¯
                    lines = entry.split("\n")
                    for line in lines:
                        line = line.strip()
                        if line.startswith("ğŸ• æ—¶é—´:"):
                            timestamp_str = line.replace("ğŸ• æ—¶é—´:", "").strip()
                            try:
                                dt = datetime.strptime(
                                    timestamp_str, "%Y-%m-%d %H:%M:%S"
                                )
                                if date_range[
                                    "start"
                                ] is None or dt < datetime.strptime(
                                    date_range["start"], "%Y-%m-%d %H:%M:%S"
                                ):
                                    date_range["start"] = timestamp_str
                                if date_range["end"] is None or dt > datetime.strptime(
                                    date_range["end"], "%Y-%m-%d %H:%M:%S"
                                ):
                                    date_range["end"] = timestamp_str
                            except ValueError:
                                pass
                        elif line.startswith("ğŸ“¡ æ¥æº:"):
                            source = line.replace("ğŸ“¡ æ¥æº:", "").strip()
                            sources.add(source)

                except Exception as e:
                    logger.debug(f"[ç¾å®³é¢„è­¦] è§£ææ—¥å¿—æ¡ç›®å¤±è´¥: {e}")
                    continue

            return {
                "enabled": self.enabled,
                "log_exists": True,
                "log_file": str(self.log_file_path),
                "total_entries": entry_count,
                "data_sources": list(sources),
                "date_range": date_range,
                "file_size_mb": file_size_mb,
                "filter_stats": self.filter_stats.copy(),
                "format_version": "2.0",  # æ ‡è®°æ–°æ ¼å¼
            }

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {e}")
            return {"enabled": self.enabled, "log_exists": False, "error": str(e)}

    def clear_logs(self):
        """æ¸…é™¤æ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
        try:
            # åˆ é™¤ä¸»æ—¥å¿—æ–‡ä»¶
            if self.log_file_path.exists():
                self.log_file_path.unlink()

            # åˆ é™¤è½®è½¬çš„æ—§æ—¥å¿—æ–‡ä»¶
            for i in range(1, self.max_files + 1):
                old_file = self.log_file_path.with_suffix(f".log.{i}")
                if old_file.exists():
                    old_file.unlink()

            logger.info("[ç¾å®³é¢„è­¦] æ‰€æœ‰æ—¥å¿—æ–‡ä»¶å·²æ¸…é™¤")

        except Exception as e:
            logger.error(f"[ç¾å®³é¢„è­¦] æ¸…é™¤æ—¥å¿—å¤±è´¥: {e}")
